{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Configuración del entorno y verificación de GPU\n",
        "\n",
        "En primer lugar se configura el entorno de ejecución para que TensorFlow utilice la GPU de forma segura, evitando errores en ciertas operaciones de convolución y permitiendo que la memoria de la GPU crezca de manera dinámica según la demanda. Luego se verifica la disponibilidad y características de la GPU mediante el comando `nvidia-smi`.\n"
      ],
      "metadata": {
        "id": "dDXsKo424jWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_strict_conv_algorithm_picker=false\"\n",
        "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\""
      ],
      "metadata": {
        "id": "jo9PaC3kuVeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23TOba33L4qf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d08508c-392a-441c-ddc0-fcae3d93c2ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov 21 00:07:53 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0             51W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalación de librerías adicionales\n",
        "\n",
        "Se instalan las librerías necesarias para el manejo de imágenes geoespaciales (`rasterio`) y para la definición de arquitecturas de segmentación profundas (`segmentation-models`). Estas librerías no vienen preinstaladas en Colab, por lo que es necesario incorporarlas explícitamente antes de construir y entrenar el modelo.\n"
      ],
      "metadata": {
        "id": "RpJoVt7F4su1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rasterio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1fR_3eaF9Wg",
        "outputId": "f153a50d-4d36-4499-f8c2-6f9ec4e542ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rasterio\n",
            "  Downloading rasterio-1.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting affine (from rasterio)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.12/dist-packages (from rasterio) (25.4.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from rasterio) (2025.10.5)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.12/dist-packages (from rasterio) (8.3.0)\n",
            "Collecting cligj>=0.5 (from rasterio)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.12/dist-packages (from rasterio) (2.0.2)\n",
            "Collecting click-plugins (from rasterio)\n",
            "  Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from rasterio) (3.2.5)\n",
            "Downloading rasterio-1.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m124.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: cligj, click-plugins, affine, rasterio\n",
            "Successfully installed affine-2.4.0 click-plugins-1.1.1.2 cligj-0.7.2 rasterio-1.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U segmentation-models==1.0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgH4TxgjKkZ-",
        "outputId": "c1c2ef33-1e97-48ad-bab5-84a1e38d837d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting segmentation-models==1.0.1\n",
            "  Downloading segmentation_models-1.0.1-py3-none-any.whl.metadata (938 bytes)\n",
            "Collecting keras-applications<=1.0.8,>=1.0.7 (from segmentation-models==1.0.1)\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting image-classifiers==1.0.0 (from segmentation-models==1.0.1)\n",
            "  Downloading image_classifiers-1.0.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting efficientnet==1.0.0 (from segmentation-models==1.0.1)\n",
            "  Downloading efficientnet-1.0.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from efficientnet==1.0.0->segmentation-models==1.0.1) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.12/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models==1.0.1) (2.0.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models==1.0.1) (3.15.1)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (1.16.3)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (2.37.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (2025.10.16)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (0.4)\n",
            "Downloading segmentation_models-1.0.1-py3-none-any.whl (33 kB)\n",
            "Downloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\n",
            "Downloading image_classifiers-1.0.0-py3-none-any.whl (19 kB)\n",
            "Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras-applications, image-classifiers, efficientnet, segmentation-models\n",
            "Successfully installed efficientnet-1.0.0 image-classifiers-1.0.0 keras-applications-1.0.8 segmentation-models-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selección de backend y acceso a Google Drive\n",
        "\n",
        "Se configura la librería `segmentation-models` para que utilice `tf.keras` como backend. Además, se monta Google Drive con el fin de:\n",
        "1. Cargar el dataset de entrenamiento/validación/test (almacenado previamente en Drive).\n",
        "2. Guardar los modelos entrenados y otros artefactos (checkpoints, logs) de forma persistente.\n"
      ],
      "metadata": {
        "id": "8yiDoySq51cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
        "os.environ[\"SM_BACKEND\"] = \"tensorflow\""
      ],
      "metadata": {
        "id": "4MM_LZbiLLUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import segmentation_models as sm\n",
        "sm.set_framework('tf.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4F3_VXXOLM4v",
        "outputId": "700f67c6-cd40-4576-eb64-75f082c2b769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Models: using `tf.keras` framework.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rtBRRkA6hip9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a18e2e6-b0f9-4118-9fa5-ae318a9bc16d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Carga y estructura del dataset de parches\n",
        "\n",
        "El dataset de entrada se encuentra comprimido en un archivo `.zip` en Google Drive. Primero se descomprime en el entorno de Colab y se define la ruta raíz del dataset (`DATASET_ROOT`), que contiene tres subcarpetas: `train`, `val` y `test`.\n",
        "\n",
        "Luego se implementa la función `get_image_mask_paths(split)` que, para un subconjunto dado (`train`, `val` o `test`), genera dos listas paralelas:\n",
        "- Rutas de las imágenes satelitales (parches GeoTIFF).\n",
        "- Rutas de las máscaras de segmentación correspondientes, construidas a partir del nombre de cada imagen.\n"
      ],
      "metadata": {
        "id": "6IvfsizU7Tnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import rasterio\n",
        "import cv2\n",
        "\n",
        "# ====== DESCOMPRIMIR ZIP EN COLAB ======\n",
        "!unzip -q \"/content/drive/MyDrive/Proyecto Integrador/dataset_final.zip\" -d \"/content\"\n",
        "\n",
        "DATASET_ROOT = \"/content/dataset_final\"  # carpeta que contiene train/valid/test\n",
        "\n",
        "print(\"Subcarpetas:\", os.listdir(DATASET_ROOT))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTAS1VsZDkxM",
        "outputId": "72fa7c4d-eb2c-4a7a-84e4-0f45026224dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subcarpetas: ['val', 'test', 'train']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_mask_paths(split):\n",
        "    img_dir = os.path.join(DATASET_ROOT, split, \"images\")\n",
        "    mask_dir = os.path.join(DATASET_ROOT, split, \"masks\")\n",
        "\n",
        "    img_paths = sorted(glob.glob(os.path.join(img_dir, \"*.tif\")))\n",
        "    mask_paths = []\n",
        "\n",
        "    for p in img_paths:\n",
        "        fname = os.path.basename(p)  # img_000001.tif\n",
        "        mask_name = fname.replace(\"img_\", \"mask_\")\n",
        "        mask_paths.append(os.path.join(mask_dir, mask_name))\n",
        "\n",
        "    print(f\"{split}: {len(img_paths)} imágenes\")\n",
        "    return img_paths, mask_paths\n",
        "\n",
        "train_img_paths, train_mask_paths = get_image_mask_paths(\"train\")\n",
        "val_img_paths,   val_mask_paths   = get_image_mask_paths(\"val\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUTse2c6FuWQ",
        "outputId": "910d2070-97a6-448e-f1da-92945323992d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: 2467 imágenes\n",
            "val: 803 imágenes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generador de datos para archivos GeoTIFF\n",
        "\n",
        "Para alimentar el modelo se define la clase `TIFFDataGenerator`, basada en `tf.keras.utils.Sequence`. Este generador:\n",
        "\n",
        "1. Recibe listas de rutas de imágenes y máscaras en formato GeoTIFF.\n",
        "2. Lee cada imagen con `rasterio` y la transforma al formato `(alto, ancho, canales)`, forzando a trabajar siempre con 3 canales (RGB).\n",
        "3. Redimensiona imágenes y máscaras al tamaño objetivo (512×512 píxeles).\n",
        "4. Normaliza cada parche de imagen mediante una escala min–max, llevando los valores al rango [0, 1].\n",
        "5. Construye la máscara en formato one-hot (tres clases: fondo, área urbana y área rural) y genera una máscara binaria de pesos (`valid_mask`) que permite ignorar los píxeles marcados con `ignore_index` (255).\n",
        "6. Devuelve, para cada batch, un triplete `(X, y, w)` compuesto por imágenes normalizadas, máscaras one-hot y pesos de muestra, listo para ser utilizado por el modelo durante el entrenamiento y la evaluación.\n"
      ],
      "metadata": {
        "id": "NCinjYltrVfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TIFFDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, image_paths, mask_paths,\n",
        "                 batch_size=4,\n",
        "                 shuffle=True,\n",
        "                 normalize=True,\n",
        "                 target_size=(512, 512),\n",
        "                 n_channels=3,      # <<< RGB\n",
        "                 n_classes=3,\n",
        "                 ignore_index=255):\n",
        "\n",
        "        self.image_paths = image_paths\n",
        "        self.mask_paths = mask_paths\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.normalize = normalize\n",
        "        self.target_size = target_size\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.ignore_index = ignore_index\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.image_paths) / self.batch_size)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indexes = np.arange(len(self.image_paths))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_indexes = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]\n",
        "        batch_imgs, batch_masks, batch_weights = [], [], []\n",
        "\n",
        "        for i in batch_indexes:\n",
        "\n",
        "            # ---------- Leer imagen ----------\n",
        "            with rasterio.open(self.image_paths[i]) as src:\n",
        "                img = src.read()               # (C, H, W)\n",
        "                img = np.transpose(img, (1, 2, 0))  # (H, W, C)\n",
        "\n",
        "            # ---------- Leer máscara ----------\n",
        "            with rasterio.open(self.mask_paths[i]) as src:\n",
        "                mask = src.read(1)\n",
        "\n",
        "            img = img.astype(np.float32)\n",
        "            mask = mask.astype(np.int32)\n",
        "\n",
        "            # ---------- Ajustar a 3 canales RGB ----------\n",
        "            if img.shape[-1] > 3:\n",
        "                img = img[..., :3]\n",
        "            elif img.shape[-1] < 3:\n",
        "                while img.shape[-1] < 3:\n",
        "                    img = np.concatenate([img, img[..., -1:]], axis=-1)\n",
        "\n",
        "            # ---------- Redimensionar ----------\n",
        "            if img.shape[0:2] != self.target_size:\n",
        "                img = cv2.resize(img, self.target_size, interpolation=cv2.INTER_LINEAR)\n",
        "            if mask.shape[0:2] != self.target_size:\n",
        "                mask = cv2.resize(mask, self.target_size, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "            # ---------- Normalización ----------\n",
        "            if self.normalize:\n",
        "                img_min, img_max = img.min(), img.max()\n",
        "                if img_max > img_min:\n",
        "                    img = (img - img_min) / (img_max - img_min)\n",
        "                else:\n",
        "                    img = np.zeros_like(img)\n",
        "\n",
        "            # ---------- sample_weights ----------\n",
        "            valid_mask = (mask != self.ignore_index).astype(\"float32\")\n",
        "\n",
        "            # ---------- Limitar mask a [0,1,2] ----------\n",
        "            mask_clipped = np.clip(mask, 0, self.n_classes - 1)\n",
        "\n",
        "            # ---------- One hot ----------\n",
        "            one_hot = np.eye(self.n_classes, dtype=\"float32\")[mask_clipped]\n",
        "\n",
        "            batch_imgs.append(img)\n",
        "            batch_masks.append(one_hot)\n",
        "            batch_weights.append(valid_mask[..., None])\n",
        "\n",
        "        X = np.stack(batch_imgs, axis=0)\n",
        "        y = np.stack(batch_masks, axis=0)\n",
        "        w = np.stack(batch_weights, axis=0)\n",
        "\n",
        "        return X, y, w"
      ],
      "metadata": {
        "id": "Fsz6_-EyFuS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definición de generadores de entrenamiento y validación\n",
        "\n",
        "Utilizando la clase `TIFFDataGenerator`, se construyen dos generadores:\n",
        "- `train_gen`, que baraja los parches en cada época (`shuffle=True`).\n",
        "- `val_gen`, que mantiene un orden fijo (`shuffle=False`) para evaluar de forma consistente.\n",
        "\n",
        "Ambos generadores producen batches de tamaño 4, con imágenes RGB de 512×512 y máscaras one-hot de tres clases (fondo, urbano, rural).\n"
      ],
      "metadata": {
        "id": "6QBmuP-277Hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen = TIFFDataGenerator(train_img_paths, train_mask_paths,\n",
        "                              batch_size=4, shuffle=True,\n",
        "                              n_channels=3, n_classes=3, ignore_index=255)\n",
        "\n",
        "val_gen = TIFFDataGenerator(val_img_paths, val_mask_paths,\n",
        "                            batch_size=4, shuffle=False,\n",
        "                            n_channels=3, n_classes=3, ignore_index=255)\n"
      ],
      "metadata": {
        "id": "2Zkf4fzP78p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelo de segmentación: U-Net con backbone ResNet34\n",
        "\n",
        "Para la tarea de segmentación semántica multiclase se utiliza la arquitectura U-Net implementada en la librería `segmentation-models`, empleando como encoder un backbone `ResNet34` preentrenado en ImageNet. El modelo trabaja con entradas RGB de 512×512 píxeles y produce, para cada píxel, una distribución de probabilidad sobre tres clases (fondo, urbano y rural) mediante una capa de salida con activación `softmax`.\n"
      ],
      "metadata": {
        "id": "MOQ43oTD79I-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "BACKBONE = 'resnet34'     # igual que tu amigo\n",
        "N_CLASSES = 3             # background, urbano, rural\n",
        "\n",
        "# ------------------------------\n",
        "# UNet + ResNet34 + ImageNet\n",
        "# ------------------------------\n",
        "model = sm.Unet(\n",
        "    backbone_name=BACKBONE,\n",
        "    encoder_weights='imagenet',           # <<< PREENTRENADO RGB\n",
        "    classes=N_CLASSES,\n",
        "    activation='softmax',                 # multiclase\n",
        "    input_shape=(512, 512, 3)             # <<< RGB\n",
        ")"
      ],
      "metadata": {
        "id": "ydN1IsFsJrvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Función de pérdida y métricas de evaluación\n",
        "\n",
        "Se definen métricas personalizadas de IoU (Intersection over Union) y F1 por clase (`fondo`, `urbano`, `rural`) a partir de las salidas one-hot del modelo. Además, se emplea una función de pérdida compuesta por:\n",
        "\n",
        "- `CategoricalFocalLoss`: que penaliza más los ejemplos difíciles y ayuda a manejar el desbalance entre clases.\n",
        "- `JaccardLoss` (basada en IoU): que optimiza directamente la superposición entre predicción y verdad terreno.\n",
        "\n",
        "El optimizador utilizado es Adam con una tasa de aprendizaje inicial de `1e-4`, y se monitorizan tanto las métricas globales (`iou_score`, `f1_score`) como las métricas por clase.\n"
      ],
      "metadata": {
        "id": "kYM-N5ba8KKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- Métricas personalizadas por clase --------------------\n",
        "import tensorflow as tf\n",
        "\n",
        "# Métrica IoU para una clase específica\n",
        "def iou_for_class(class_id):\n",
        "    def metric(y_true, y_pred):\n",
        "        y_true_c = tf.cast(tf.equal(tf.argmax(y_true, -1), class_id), tf.float32)\n",
        "        y_pred_c = tf.cast(tf.equal(tf.argmax(y_pred, -1), class_id), tf.float32)\n",
        "        inter = tf.reduce_sum(y_true_c * y_pred_c)\n",
        "        union = tf.reduce_sum(y_true_c) + tf.reduce_sum(y_pred_c) - inter + 1e-7\n",
        "        return inter / union\n",
        "    metric.__name__ = f'iou_class_{class_id}'\n",
        "    return metric\n",
        "\n",
        "# Métrica F1 para una clase\n",
        "def f1_for_class(class_id):\n",
        "    def metric(y_true, y_pred):\n",
        "        y_true_c = tf.cast(tf.equal(tf.argmax(y_true, -1), class_id), tf.float32)\n",
        "        y_pred_c = tf.cast(tf.equal(tf.argmax(y_pred, -1), class_id), tf.float32)\n",
        "        tp = tf.reduce_sum(y_true_c * y_pred_c)\n",
        "        fp = tf.reduce_sum((1 - y_true_c) * y_pred_c)\n",
        "        fn = tf.reduce_sum(y_true_c * (1 - y_pred_c))\n",
        "        precision = tp / (tp + fp + 1e-7)\n",
        "        recall    = tp / (tp + fn + 1e-7)\n",
        "        return 2 * precision * recall / (precision + recall + 1e-7)\n",
        "    metric.__name__ = f'f1_class_{class_id}'\n",
        "    return metric\n",
        "\n",
        "# -------------------- Pérdida y métricas globales --------------------\n",
        "loss = sm.losses.CategoricalFocalLoss() + sm.losses.JaccardLoss()\n",
        "\n",
        "metrics = [\n",
        "    sm.metrics.IOUScore(threshold=None, name=\"iou_score\"),\n",
        "    sm.metrics.FScore(threshold=None, name=\"f1_score\"),\n",
        "    iou_for_class(0),  # background\n",
        "    iou_for_class(1),  # urbano\n",
        "    iou_for_class(2),  # rural\n",
        "    f1_for_class(0),\n",
        "    f1_for_class(1),\n",
        "    f1_for_class(2),\n",
        "]"
      ],
      "metadata": {
        "id": "HFBiv9dxFuPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=Adam(1e-4),\n",
        "    loss=loss,\n",
        "    metrics=[\n",
        "        sm.metrics.IOUScore(threshold=None, name=\"iou_score\"),\n",
        "        sm.metrics.FScore(threshold=None, name=\"f1_score\"),\n",
        "\n",
        "        iou_for_class(0),\n",
        "        iou_for_class(1),\n",
        "        iou_for_class(2),\n",
        "\n",
        "        f1_for_class(0),\n",
        "        f1_for_class(1),\n",
        "        f1_for_class(2),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "vssf05CdKJfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Callbacks: parada temprana y guardado de modelos\n",
        "\n",
        "Para controlar el entrenamiento se emplean varios callbacks:\n",
        "\n",
        "- **EarlyStopping**: detiene el entrenamiento cuando la métrica `val_iou_score` deja de mejorar durante un número determinado de épocas, restaurando los mejores pesos observados.\n",
        "- **ModelCheckpoint (mejor modelo)**: guarda en Google Drive el modelo con el mayor `val_iou_score`.\n",
        "- **ModelCheckpoint (por época)**: almacena un checkpoint adicional por cada época, permitiendo revisar versiones intermedias del modelo.\n",
        "- **TensorBoard**: registra los logs de entrenamiento y validación para su posterior visualización gráfica (pérdidas, métricas, etc.).\n"
      ],
      "metadata": {
        "id": "3_PsboqN8RO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/Proyecto Integrador/ccpp_checkpoints\"\n",
        "\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(\"logs\", exist_ok=True)\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_iou_score',\n",
        "    mode='max',\n",
        "    patience=15,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "checkpoint_best = ModelCheckpoint(\n",
        "    filepath=os.path.join(CHECKPOINT_DIR, \"best_unet_multiclass.keras\"),\n",
        "    monitor='val_iou_score',\n",
        "    mode='max',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "checkpoint_every = ModelCheckpoint(\n",
        "    filepath=\"checkpoints/ckpt_epoch_{epoch:02d}.keras\",\n",
        "    save_best_only=False,\n",
        "    save_weights_only=False,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "tensorboard_cb = TensorBoard(\n",
        "    log_dir=\"logs/unet_multiclass\",\n",
        "    histogram_freq=0\n",
        ")\n"
      ],
      "metadata": {
        "id": "lkaXe6rJFuMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamiento del modelo\n",
        "\n",
        "El modelo se entrena utilizando los generadores de entrenamiento y validación durante un máximo de 60 épocas. El proceso está controlado por la parada temprana, de modo que el entrenamiento se detiene automáticamente cuando la métrica `val_iou_score` deja de mejorar, evitando sobreajuste. Al finalizar, se dispone del mejor modelo según el desempeño en el conjunto de validación.\n"
      ],
      "metadata": {
        "id": "j3jIgrhA8T5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 60\n",
        "\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stop, checkpoint_best, checkpoint_every, tensorboard_cb]\n",
        ")\n",
        "\n",
        "print(\"Epochs realmente entrenadas:\", len(history.history['loss']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lljjhRrpDktZ",
        "outputId": "56932fa3-1693-4291-9c67-3a4130645404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - f1_class_0: 0.9071 - f1_class_1: 0.0559 - f1_class_2: 0.3150 - f1_score: 0.3148 - iou_class_0: 0.8546 - iou_class_1: 0.0426 - iou_class_2: 0.2084 - iou_score: 0.2503 - loss: 0.7703\n",
            "Epoch 1: val_iou_score improved from -inf to 0.30866, saving model to /content/drive/MyDrive/Proyecto Integrador/ccpp_checkpoints/best_unet_multiclass.keras\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 163ms/step - f1_class_0: 0.9072 - f1_class_1: 0.0560 - f1_class_2: 0.3152 - f1_score: 0.3149 - iou_class_0: 0.8547 - iou_class_1: 0.0426 - iou_class_2: 0.2086 - iou_score: 0.2505 - loss: 0.7701 - val_f1_class_0: 0.9661 - val_f1_class_1: 0.0000e+00 - val_f1_class_2: 0.0000e+00 - val_f1_score: 0.3226 - val_iou_class_0: 0.9439 - val_iou_class_1: 0.0000e+00 - val_iou_class_2: 0.0000e+00 - val_iou_score: 0.3087 - val_loss: 0.7105\n",
            "Epoch 2/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - f1_class_0: 0.9867 - f1_class_1: 0.1660 - f1_class_2: 0.5806 - f1_score: 0.5486 - iou_class_0: 0.9743 - iou_class_1: 0.1433 - iou_class_2: 0.4296 - iou_score: 0.4836 - loss: 0.5249\n",
            "Epoch 2: val_iou_score improved from 0.30866 to 0.47111, saving model to /content/drive/MyDrive/Proyecto Integrador/ccpp_checkpoints/best_unet_multiclass.keras\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 109ms/step - f1_class_0: 0.9867 - f1_class_1: 0.1660 - f1_class_2: 0.5806 - f1_score: 0.5486 - iou_class_0: 0.9743 - iou_class_1: 0.1434 - iou_class_2: 0.4297 - iou_score: 0.4837 - loss: 0.5249 - val_f1_class_0: 0.9813 - val_f1_class_1: 0.1362 - val_f1_class_2: 0.5011 - val_f1_score: 0.5301 - val_iou_class_0: 0.9655 - val_iou_class_1: 0.1131 - val_iou_class_2: 0.3646 - val_iou_score: 0.4711 - val_loss: 0.5421\n",
            "Epoch 3/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - f1_class_0: 0.9901 - f1_class_1: 0.1889 - f1_class_2: 0.6918 - f1_score: 0.6147 - iou_class_0: 0.9810 - iou_class_1: 0.1659 - iou_class_2: 0.5417 - iou_score: 0.5520 - loss: 0.4565\n",
            "Epoch 3: val_iou_score improved from 0.47111 to 0.48836, saving model to /content/drive/MyDrive/Proyecto Integrador/ccpp_checkpoints/best_unet_multiclass.keras\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 107ms/step - f1_class_0: 0.9901 - f1_class_1: 0.1890 - f1_class_2: 0.6918 - f1_score: 0.6147 - iou_class_0: 0.9810 - iou_class_1: 0.1659 - iou_class_2: 0.5417 - iou_score: 0.5520 - loss: 0.4565 - val_f1_class_0: 0.9872 - val_f1_class_1: 0.1373 - val_f1_class_2: 0.5265 - val_f1_score: 0.5471 - val_iou_class_0: 0.9756 - val_iou_class_1: 0.1174 - val_iou_class_2: 0.3834 - val_iou_score: 0.4884 - val_loss: 0.5221\n",
            "Epoch 4/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - f1_class_0: 0.9932 - f1_class_1: 0.2341 - f1_class_2: 0.7405 - f1_score: 0.6513 - iou_class_0: 0.9866 - iou_class_1: 0.2118 - iou_class_2: 0.5985 - iou_score: 0.5929 - loss: 0.4134\n",
            "Epoch 4: val_iou_score did not improve from 0.48836\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 107ms/step - f1_class_0: 0.9932 - f1_class_1: 0.2340 - f1_class_2: 0.7405 - f1_score: 0.6513 - iou_class_0: 0.9866 - iou_class_1: 0.2118 - iou_class_2: 0.5986 - iou_score: 0.5929 - loss: 0.4134 - val_f1_class_0: 0.9859 - val_f1_class_1: 0.1302 - val_f1_class_2: 0.4913 - val_f1_score: 0.5342 - val_iou_class_0: 0.9735 - val_iou_class_1: 0.1105 - val_iou_class_2: 0.3590 - val_iou_score: 0.4791 - val_loss: 0.5338\n",
            "Epoch 5/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - f1_class_0: 0.9952 - f1_class_1: 0.2461 - f1_class_2: 0.7732 - f1_score: 0.6682 - iou_class_0: 0.9906 - iou_class_1: 0.2270 - iou_class_2: 0.6423 - iou_score: 0.6155 - loss: 0.3893\n",
            "Epoch 5: val_iou_score did not improve from 0.48836\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 106ms/step - f1_class_0: 0.9952 - f1_class_1: 0.2461 - f1_class_2: 0.7732 - f1_score: 0.6682 - iou_class_0: 0.9906 - iou_class_1: 0.2271 - iou_class_2: 0.6423 - iou_score: 0.6155 - loss: 0.3893 - val_f1_class_0: 0.9870 - val_f1_class_1: 0.1335 - val_f1_class_2: 0.4689 - val_f1_score: 0.5288 - val_iou_class_0: 0.9755 - val_iou_class_1: 0.1120 - val_iou_class_2: 0.3392 - val_iou_score: 0.4744 - val_loss: 0.5401\n",
            "Epoch 6/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - f1_class_0: 0.9956 - f1_class_1: 0.2236 - f1_class_2: 0.7974 - f1_score: 0.6704 - iou_class_0: 0.9913 - iou_class_1: 0.2073 - iou_class_2: 0.6730 - iou_score: 0.6212 - loss: 0.3832\n",
            "Epoch 6: val_iou_score did not improve from 0.48836\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 104ms/step - f1_class_0: 0.9956 - f1_class_1: 0.2236 - f1_class_2: 0.7975 - f1_score: 0.6704 - iou_class_0: 0.9913 - iou_class_1: 0.2074 - iou_class_2: 0.6730 - iou_score: 0.6213 - loss: 0.3832 - val_f1_class_0: 0.9847 - val_f1_class_1: 0.1305 - val_f1_class_2: 0.4822 - val_f1_score: 0.5318 - val_iou_class_0: 0.9714 - val_iou_class_1: 0.1088 - val_iou_class_2: 0.3535 - val_iou_score: 0.4771 - val_loss: 0.5386\n",
            "Epoch 7/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - f1_class_0: 0.9955 - f1_class_1: 0.2669 - f1_class_2: 0.8168 - f1_score: 0.6914 - iou_class_0: 0.9911 - iou_class_1: 0.2477 - iou_class_2: 0.6995 - iou_score: 0.6436 - loss: 0.3612\n",
            "Epoch 7: val_iou_score did not improve from 0.48836\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 105ms/step - f1_class_0: 0.9955 - f1_class_1: 0.2669 - f1_class_2: 0.8168 - f1_score: 0.6913 - iou_class_0: 0.9911 - iou_class_1: 0.2476 - iou_class_2: 0.6995 - iou_score: 0.6436 - loss: 0.3612 - val_f1_class_0: 0.9879 - val_f1_class_1: 0.1238 - val_f1_class_2: 0.5105 - val_f1_score: 0.5401 - val_iou_class_0: 0.9770 - val_iou_class_1: 0.1059 - val_iou_class_2: 0.3793 - val_iou_score: 0.4867 - val_loss: 0.5267\n",
            "Epoch 8/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - f1_class_0: 0.9966 - f1_class_1: 0.2551 - f1_class_2: 0.8376 - f1_score: 0.6949 - iou_class_0: 0.9932 - iou_class_1: 0.2393 - iou_class_2: 0.7254 - iou_score: 0.6504 - loss: 0.3530\n",
            "Epoch 8: val_iou_score did not improve from 0.48836\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 109ms/step - f1_class_0: 0.9966 - f1_class_1: 0.2551 - f1_class_2: 0.8376 - f1_score: 0.6949 - iou_class_0: 0.9932 - iou_class_1: 0.2393 - iou_class_2: 0.7254 - iou_score: 0.6504 - loss: 0.3530 - val_f1_class_0: 0.9852 - val_f1_class_1: 0.1099 - val_f1_class_2: 0.4613 - val_f1_score: 0.5184 - val_iou_class_0: 0.9724 - val_iou_class_1: 0.0931 - val_iou_class_2: 0.3357 - val_iou_score: 0.4666 - val_loss: 0.5510\n",
            "Epoch 9/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - f1_class_0: 0.9952 - f1_class_1: 0.2497 - f1_class_2: 0.8343 - f1_score: 0.6920 - iou_class_0: 0.9911 - iou_class_1: 0.2305 - iou_class_2: 0.7242 - iou_score: 0.6469 - loss: 0.3580\n",
            "Epoch 9: val_iou_score improved from 0.48836 to 0.49308, saving model to /content/drive/MyDrive/Proyecto Integrador/ccpp_checkpoints/best_unet_multiclass.keras\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 106ms/step - f1_class_0: 0.9952 - f1_class_1: 0.2497 - f1_class_2: 0.8343 - f1_score: 0.6920 - iou_class_0: 0.9911 - iou_class_1: 0.2305 - iou_class_2: 0.7242 - iou_score: 0.6470 - loss: 0.3579 - val_f1_class_0: 0.9875 - val_f1_class_1: 0.1253 - val_f1_class_2: 0.5304 - val_f1_score: 0.5473 - val_iou_class_0: 0.9764 - val_iou_class_1: 0.1080 - val_iou_class_2: 0.3963 - val_iou_score: 0.4931 - val_loss: 0.5220\n",
            "Epoch 10/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - f1_class_0: 0.9970 - f1_class_1: 0.2530 - f1_class_2: 0.8600 - f1_score: 0.7024 - iou_class_0: 0.9941 - iou_class_1: 0.2403 - iou_class_2: 0.7596 - iou_score: 0.6631 - loss: 0.3400\n",
            "Epoch 10: val_iou_score did not improve from 0.49308\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 107ms/step - f1_class_0: 0.9970 - f1_class_1: 0.2530 - f1_class_2: 0.8600 - f1_score: 0.7024 - iou_class_0: 0.9941 - iou_class_1: 0.2403 - iou_class_2: 0.7595 - iou_score: 0.6631 - loss: 0.3400 - val_f1_class_0: 0.9858 - val_f1_class_1: 0.1383 - val_f1_class_2: 0.4830 - val_f1_score: 0.5353 - val_iou_class_0: 0.9733 - val_iou_class_1: 0.1165 - val_iou_class_2: 0.3567 - val_iou_score: 0.4818 - val_loss: 0.5357\n",
            "Epoch 11/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - f1_class_0: 0.9962 - f1_class_1: 0.2615 - f1_class_2: 0.8392 - f1_score: 0.6981 - iou_class_0: 0.9925 - iou_class_1: 0.2459 - iou_class_2: 0.7321 - iou_score: 0.6556 - loss: 0.3489\n",
            "Epoch 11: val_iou_score improved from 0.49308 to 0.49587, saving model to /content/drive/MyDrive/Proyecto Integrador/ccpp_checkpoints/best_unet_multiclass.keras\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 116ms/step - f1_class_0: 0.9962 - f1_class_1: 0.2615 - f1_class_2: 0.8392 - f1_score: 0.6981 - iou_class_0: 0.9925 - iou_class_1: 0.2460 - iou_class_2: 0.7321 - iou_score: 0.6556 - loss: 0.3489 - val_f1_class_0: 0.9884 - val_f1_class_1: 0.1334 - val_f1_class_2: 0.5301 - val_f1_score: 0.5503 - val_iou_class_0: 0.9780 - val_iou_class_1: 0.1135 - val_iou_class_2: 0.3972 - val_iou_score: 0.4959 - val_loss: 0.5193\n",
            "Epoch 12/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - f1_class_0: 0.9973 - f1_class_1: 0.2507 - f1_class_2: 0.8672 - f1_score: 0.7041 - iou_class_0: 0.9947 - iou_class_1: 0.2368 - iou_class_2: 0.7707 - iou_score: 0.6660 - loss: 0.3368\n",
            "Epoch 12: val_iou_score did not improve from 0.49587\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 107ms/step - f1_class_0: 0.9973 - f1_class_1: 0.2508 - f1_class_2: 0.8672 - f1_score: 0.7041 - iou_class_0: 0.9947 - iou_class_1: 0.2368 - iou_class_2: 0.7708 - iou_score: 0.6660 - loss: 0.3368 - val_f1_class_0: 0.9882 - val_f1_class_1: 0.1293 - val_f1_class_2: 0.4945 - val_f1_score: 0.5370 - val_iou_class_0: 0.9777 - val_iou_class_1: 0.1098 - val_iou_class_2: 0.3637 - val_iou_score: 0.4833 - val_loss: 0.5323\n",
            "Epoch 13/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - f1_class_0: 0.9972 - f1_class_1: 0.2665 - f1_class_2: 0.8743 - f1_score: 0.7119 - iou_class_0: 0.9945 - iou_class_1: 0.2514 - iou_class_2: 0.7804 - iou_score: 0.6743 - loss: 0.3287\n",
            "Epoch 13: val_iou_score did not improve from 0.49587\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 105ms/step - f1_class_0: 0.9972 - f1_class_1: 0.2665 - f1_class_2: 0.8743 - f1_score: 0.7120 - iou_class_0: 0.9945 - iou_class_1: 0.2514 - iou_class_2: 0.7804 - iou_score: 0.6743 - loss: 0.3287 - val_f1_class_0: 0.9880 - val_f1_class_1: 0.1112 - val_f1_class_2: 0.4347 - val_f1_score: 0.5112 - val_iou_class_0: 0.9773 - val_iou_class_1: 0.0962 - val_iou_class_2: 0.3160 - val_iou_score: 0.4630 - val_loss: 0.5549\n",
            "Epoch 14/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - f1_class_0: 0.9977 - f1_class_1: 0.2795 - f1_class_2: 0.8850 - f1_score: 0.7201 - iou_class_0: 0.9954 - iou_class_1: 0.2684 - iou_class_2: 0.7987 - iou_score: 0.6865 - loss: 0.3159\n",
            "Epoch 14: val_iou_score did not improve from 0.49587\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 115ms/step - f1_class_0: 0.9977 - f1_class_1: 0.2795 - f1_class_2: 0.8850 - f1_score: 0.7201 - iou_class_0: 0.9954 - iou_class_1: 0.2685 - iou_class_2: 0.7987 - iou_score: 0.6865 - loss: 0.3159 - val_f1_class_0: 0.9874 - val_f1_class_1: 0.1328 - val_f1_class_2: 0.4222 - val_f1_score: 0.5140 - val_iou_class_0: 0.9763 - val_iou_class_1: 0.1117 - val_iou_class_2: 0.3045 - val_iou_score: 0.4639 - val_loss: 0.5555\n",
            "Epoch 15/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - f1_class_0: 0.9964 - f1_class_1: 0.2568 - f1_class_2: 0.8733 - f1_score: 0.7081 - iou_class_0: 0.9930 - iou_class_1: 0.2390 - iou_class_2: 0.7789 - iou_score: 0.6691 - loss: 0.3348\n",
            "Epoch 15: val_iou_score did not improve from 0.49587\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 107ms/step - f1_class_0: 0.9964 - f1_class_1: 0.2568 - f1_class_2: 0.8733 - f1_score: 0.7081 - iou_class_0: 0.9930 - iou_class_1: 0.2390 - iou_class_2: 0.7789 - iou_score: 0.6692 - loss: 0.3348 - val_f1_class_0: 0.9882 - val_f1_class_1: 0.1421 - val_f1_class_2: 0.4757 - val_f1_score: 0.5350 - val_iou_class_0: 0.9777 - val_iou_class_1: 0.1203 - val_iou_class_2: 0.3519 - val_iou_score: 0.4829 - val_loss: 0.5348\n",
            "Epoch 16/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - f1_class_0: 0.9978 - f1_class_1: 0.2765 - f1_class_2: 0.8872 - f1_score: 0.7197 - iou_class_0: 0.9956 - iou_class_1: 0.2665 - iou_class_2: 0.8011 - iou_score: 0.6865 - loss: 0.3159\n",
            "Epoch 16: val_iou_score did not improve from 0.49587\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 112ms/step - f1_class_0: 0.9978 - f1_class_1: 0.2765 - f1_class_2: 0.8871 - f1_score: 0.7197 - iou_class_0: 0.9956 - iou_class_1: 0.2665 - iou_class_2: 0.8011 - iou_score: 0.6865 - loss: 0.3159 - val_f1_class_0: 0.9872 - val_f1_class_1: 0.1230 - val_f1_class_2: 0.5035 - val_f1_score: 0.5376 - val_iou_class_0: 0.9756 - val_iou_class_1: 0.1050 - val_iou_class_2: 0.3648 - val_iou_score: 0.4816 - val_loss: 0.5379\n",
            "Epoch 17/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - f1_class_0: 0.9974 - f1_class_1: 0.2969 - f1_class_2: 0.8824 - f1_score: 0.7250 - iou_class_0: 0.9949 - iou_class_1: 0.2840 - iou_class_2: 0.7945 - iou_score: 0.6902 - loss: 0.3127\n",
            "Epoch 17: val_iou_score did not improve from 0.49587\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 110ms/step - f1_class_0: 0.9974 - f1_class_1: 0.2969 - f1_class_2: 0.8824 - f1_score: 0.7250 - iou_class_0: 0.9949 - iou_class_1: 0.2840 - iou_class_2: 0.7945 - iou_score: 0.6902 - loss: 0.3127 - val_f1_class_0: 0.9886 - val_f1_class_1: 0.1193 - val_f1_class_2: 0.5054 - val_f1_score: 0.5375 - val_iou_class_0: 0.9784 - val_iou_class_1: 0.1037 - val_iou_class_2: 0.3765 - val_iou_score: 0.4858 - val_loss: 0.5318\n",
            "Epoch 18/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - f1_class_0: 0.9982 - f1_class_1: 0.3109 - f1_class_2: 0.9033 - f1_score: 0.7369 - iou_class_0: 0.9964 - iou_class_1: 0.3008 - iou_class_2: 0.8266 - iou_score: 0.7070 - loss: 0.2949\n",
            "Epoch 18: val_iou_score did not improve from 0.49587\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 106ms/step - f1_class_0: 0.9982 - f1_class_1: 0.3108 - f1_class_2: 0.9033 - f1_score: 0.7369 - iou_class_0: 0.9964 - iou_class_1: 0.3007 - iou_class_2: 0.8266 - iou_score: 0.7070 - loss: 0.2949 - val_f1_class_0: 0.9882 - val_f1_class_1: 0.1231 - val_f1_class_2: 0.5286 - val_f1_score: 0.5464 - val_iou_class_0: 0.9777 - val_iou_class_1: 0.1057 - val_iou_class_2: 0.3937 - val_iou_score: 0.4921 - val_loss: 0.5255\n",
            "Epoch 19/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - f1_class_0: 0.9974 - f1_class_1: 0.3148 - f1_class_2: 0.8942 - f1_score: 0.7349 - iou_class_0: 0.9949 - iou_class_1: 0.3030 - iou_class_2: 0.8120 - iou_score: 0.7024 - loss: 0.3003\n",
            "Epoch 19: val_iou_score did not improve from 0.49587\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 110ms/step - f1_class_0: 0.9974 - f1_class_1: 0.3147 - f1_class_2: 0.8942 - f1_score: 0.7349 - iou_class_0: 0.9949 - iou_class_1: 0.3030 - iou_class_2: 0.8120 - iou_score: 0.7024 - loss: 0.3003 - val_f1_class_0: 0.9887 - val_f1_class_1: 0.1337 - val_f1_class_2: 0.4918 - val_f1_score: 0.5379 - val_iou_class_0: 0.9785 - val_iou_class_1: 0.1144 - val_iou_class_2: 0.3722 - val_iou_score: 0.4881 - val_loss: 0.5288\n",
            "Epoch 20/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - f1_class_0: 0.9981 - f1_class_1: 0.3064 - f1_class_2: 0.9086 - f1_score: 0.7373 - iou_class_0: 0.9963 - iou_class_1: 0.2977 - iou_class_2: 0.8353 - iou_score: 0.7090 - loss: 0.2932\n",
            "Epoch 20: val_iou_score did not improve from 0.49587\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 122ms/step - f1_class_0: 0.9981 - f1_class_1: 0.3064 - f1_class_2: 0.9086 - f1_score: 0.7373 - iou_class_0: 0.9963 - iou_class_1: 0.2976 - iou_class_2: 0.8353 - iou_score: 0.7089 - loss: 0.2932 - val_f1_class_0: 0.9887 - val_f1_class_1: 0.1293 - val_f1_class_2: 0.5015 - val_f1_score: 0.5397 - val_iou_class_0: 0.9786 - val_iou_class_1: 0.1112 - val_iou_class_2: 0.3720 - val_iou_score: 0.4871 - val_loss: 0.5314\n",
            "Epoch 21/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - f1_class_0: 0.9984 - f1_class_1: 0.2736 - f1_class_2: 0.9087 - f1_score: 0.7265 - iou_class_0: 0.9968 - iou_class_1: 0.2655 - iou_class_2: 0.8361 - iou_score: 0.6987 - loss: 0.3030\n",
            "Epoch 21: val_iou_score did not improve from 0.49587\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 109ms/step - f1_class_0: 0.9984 - f1_class_1: 0.2736 - f1_class_2: 0.9087 - f1_score: 0.7265 - iou_class_0: 0.9968 - iou_class_1: 0.2655 - iou_class_2: 0.8361 - iou_score: 0.6987 - loss: 0.3030 - val_f1_class_0: 0.9886 - val_f1_class_1: 0.1088 - val_f1_class_2: 0.4316 - val_f1_score: 0.5095 - val_iou_class_0: 0.9784 - val_iou_class_1: 0.0959 - val_iou_class_2: 0.3061 - val_iou_score: 0.4600 - val_loss: 0.5615\n",
            "Epoch 22/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - f1_class_0: 0.9982 - f1_class_1: 0.2868 - f1_class_2: 0.9166 - f1_score: 0.7334 - iou_class_0: 0.9966 - iou_class_1: 0.2795 - iou_class_2: 0.8483 - iou_score: 0.7074 - loss: 0.2946\n",
            "Epoch 22: val_iou_score did not improve from 0.49587\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 107ms/step - f1_class_0: 0.9982 - f1_class_1: 0.2868 - f1_class_2: 0.9165 - f1_score: 0.7334 - iou_class_0: 0.9965 - iou_class_1: 0.2795 - iou_class_2: 0.8483 - iou_score: 0.7074 - loss: 0.2946 - val_f1_class_0: 0.9871 - val_f1_class_1: 0.1203 - val_f1_class_2: 0.3922 - val_f1_score: 0.4997 - val_iou_class_0: 0.9758 - val_iou_class_1: 0.1022 - val_iou_class_2: 0.2778 - val_iou_score: 0.4518 - val_loss: 0.5676\n",
            "Epoch 23/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - f1_class_0: 0.9972 - f1_class_1: 0.2933 - f1_class_2: 0.8857 - f1_score: 0.7248 - iou_class_0: 0.9945 - iou_class_1: 0.2811 - iou_class_2: 0.8030 - iou_score: 0.6919 - loss: 0.3109\n",
            "Epoch 23: val_iou_score did not improve from 0.49587\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 106ms/step - f1_class_0: 0.9972 - f1_class_1: 0.2933 - f1_class_2: 0.8857 - f1_score: 0.7248 - iou_class_0: 0.9945 - iou_class_1: 0.2811 - iou_class_2: 0.8031 - iou_score: 0.6920 - loss: 0.3109 - val_f1_class_0: 0.9892 - val_f1_class_1: 0.1396 - val_f1_class_2: 0.4867 - val_f1_score: 0.5383 - val_iou_class_0: 0.9795 - val_iou_class_1: 0.1197 - val_iou_class_2: 0.3580 - val_iou_score: 0.4855 - val_loss: 0.5319\n",
            "Epoch 24/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - f1_class_0: 0.9983 - f1_class_1: 0.2813 - f1_class_2: 0.9114 - f1_score: 0.7299 - iou_class_0: 0.9966 - iou_class_1: 0.2727 - iou_class_2: 0.8412 - iou_score: 0.7028 - loss: 0.2994\n",
            "Epoch 24: val_iou_score did not improve from 0.49587\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 106ms/step - f1_class_0: 0.9983 - f1_class_1: 0.2813 - f1_class_2: 0.9114 - f1_score: 0.7299 - iou_class_0: 0.9966 - iou_class_1: 0.2728 - iou_class_2: 0.8411 - iou_score: 0.7028 - loss: 0.2994 - val_f1_class_0: 0.9882 - val_f1_class_1: 0.1074 - val_f1_class_2: 0.4892 - val_f1_score: 0.5281 - val_iou_class_0: 0.9777 - val_iou_class_1: 0.0936 - val_iou_class_2: 0.3619 - val_iou_score: 0.4776 - val_loss: 0.5430\n",
            "Epoch 25/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - f1_class_0: 0.9983 - f1_class_1: 0.2709 - f1_class_2: 0.9141 - f1_score: 0.7273 - iou_class_0: 0.9965 - iou_class_1: 0.2631 - iou_class_2: 0.8443 - iou_score: 0.7006 - loss: 0.3012\n",
            "Epoch 25: val_iou_score did not improve from 0.49587\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 104ms/step - f1_class_0: 0.9983 - f1_class_1: 0.2709 - f1_class_2: 0.9141 - f1_score: 0.7273 - iou_class_0: 0.9965 - iou_class_1: 0.2631 - iou_class_2: 0.8443 - iou_score: 0.7006 - loss: 0.3012 - val_f1_class_0: 0.9889 - val_f1_class_1: 0.1264 - val_f1_class_2: 0.4831 - val_f1_score: 0.5327 - val_iou_class_0: 0.9790 - val_iou_class_1: 0.1100 - val_iou_class_2: 0.3565 - val_iou_score: 0.4817 - val_loss: 0.5364\n",
            "Epoch 26/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - f1_class_0: 0.9986 - f1_class_1: 0.3045 - f1_class_2: 0.9259 - f1_score: 0.7427 - iou_class_0: 0.9973 - iou_class_1: 0.2979 - iou_class_2: 0.8640 - iou_score: 0.7191 - loss: 0.2823\n",
            "Epoch 26: val_iou_score did not improve from 0.49587\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 108ms/step - f1_class_0: 0.9986 - f1_class_1: 0.3045 - f1_class_2: 0.9259 - f1_score: 0.7427 - iou_class_0: 0.9973 - iou_class_1: 0.2979 - iou_class_2: 0.8640 - iou_score: 0.7191 - loss: 0.2823 - val_f1_class_0: 0.9884 - val_f1_class_1: 0.1280 - val_f1_class_2: 0.5166 - val_f1_score: 0.5442 - val_iou_class_0: 0.9781 - val_iou_class_1: 0.1100 - val_iou_class_2: 0.3818 - val_iou_score: 0.4898 - val_loss: 0.5297\n",
            "Epoch 26: early stopping\n",
            "Restoring model weights from the end of the best epoch: 11.\n",
            "Epochs realmente entrenadas: 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recarga y afinamiento adicional del mejor modelo\n",
        "\n",
        "En caso de ser necesario, se recarga explícitamente el mejor modelo guardado mediante `ModelCheckpoint`, especificando las funciones de pérdida y métricas personalizadas (`custom_objects`). A partir de este punto es posible realizar un afinamiento adicional (fine-tuning) sobre el mismo conjunto de entrenamiento y validación, manteniendo los mismos callbacks y criterios de parada.\n"
      ],
      "metadata": {
        "id": "q6sPCVOI8fKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import segmentation_models as sm\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
        "sm.set_framework('tf.keras')\n",
        "\n",
        "custom_objects = {\n",
        "    'CategoricalFocalJaccardLoss': sm.losses.CategoricalFocalJaccardLoss,\n",
        "    'iou_score': sm.metrics.IOUScore,\n",
        "    'f1_score': sm.metrics.FScore,\n",
        "    # métricas custom:\n",
        "    'iou_class_0': iou_for_class(0),\n",
        "    'iou_class_1': iou_for_class(1),\n",
        "    'iou_class_2': iou_for_class(2),\n",
        "    'f1_class_0':  f1_for_class(0),\n",
        "    'f1_class_1':  f1_for_class(1),\n",
        "    'f1_class_2':  f1_for_class(2),\n",
        "}\n",
        "\n",
        "ckpt_path = \"checkpoints/best_unet_multiclass.keras\"  # o uno de los ckpt_epoch_XX\n",
        "assert os.path.exists(ckpt_path), \"No existe el checkpoint elegido\"\n",
        "\n",
        "model = tf.keras.models.load_model(ckpt_path, custom_objects=custom_objects)\n",
        "print(\"Modelo cargado desde:\", ckpt_path)\n"
      ],
      "metadata": {
        "id": "7w296YrzDkqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history2 = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=20,   # epochs adicionales\n",
        "    callbacks=[early_stop, checkpoint_best, checkpoint_every, tensorboard_cb]\n",
        ")\n"
      ],
      "metadata": {
        "id": "4DETd65DDkbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluación en el conjunto de prueba\n",
        "\n",
        "Para medir el desempeño final del modelo se construye un generador específico para el conjunto de prueba (`test_gen`), que recorre todas las imágenes de test con `batch_size=1`. A continuación, se utiliza `model.evaluate` para obtener la pérdida y las métricas definidas (IoU y F1 globales y por clase), proporcionando una estimación objetiva de la capacidad de generalización del modelo sobre parches no vistos durante el entrenamiento ni la validación.\n"
      ],
      "metadata": {
        "id": "8rKZd4sX8lGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "TEST_IMG_DIR = os.path.join(DATASET_ROOT, \"test/images\")\n",
        "TEST_MASK_DIR = os.path.join(DATASET_ROOT, \"test/masks\")\n",
        "\n",
        "test_img_paths = sorted(glob.glob(os.path.join(TEST_IMG_DIR, \"*.tif\")))\n",
        "test_mask_paths = sorted(glob.glob(os.path.join(TEST_MASK_DIR, \"*.tif\")))\n",
        "\n",
        "print(\"Test imágenes:\", len(test_img_paths))\n",
        "print(\"Test máscaras:\", len(test_mask_paths))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4yZPE0AFvwM",
        "outputId": "02c3b74d-b45e-46a0-bceb-93ec9149e74b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test imágenes: 479\n",
            "Test máscaras: 479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_gen = TIFFDataGenerator(\n",
        "    test_img_paths,\n",
        "    test_mask_paths,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    target_size=(512,512),\n",
        "    n_channels=3,\n",
        "    n_classes=3\n",
        ")"
      ],
      "metadata": {
        "id": "BY36wbCKFvs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics = model.evaluate(test_gen, verbose=1)\n",
        "for name, value in zip(model.metrics_names, test_metrics):\n",
        "    print(f\"{name}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZdX6TqpFvps",
        "outputId": "373a5a7f-e188-4ed3-ba46-6697e7ec6e74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step - f1_class_0: 0.9970 - f1_class_1: 0.0000e+00 - f1_class_2: 0.5994 - f1_score: 0.5316 - iou_class_0: 0.9940 - iou_class_1: 0.0000e+00 - iou_class_2: 0.4728 - iou_score: 0.4883 - loss: 0.5165\n",
            "loss: 0.5199595093727112\n",
            "compile_metrics: 0.48464134335517883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicción preliminar (sanity check) sobre un parche de test\n",
        "\n",
        "Antes de generar y guardar las predicciones completas, se realiza una prueba rápida\n",
        "(sanity check) tomando el primer batch del conjunto de prueba. Esto permite verificar\n",
        "que:\n",
        "- El generador produce imágenes y máscaras correctamente procesadas.\n",
        "- El modelo devuelve probabilidades softmax con forma `(512,512,3)`.\n",
        "- La conversión a clases discretas mediante `argmax` produce una máscara de clases\n",
        "de forma `(512,512)`.\n",
        "\n",
        "Esta verificación asegura que todo el pipeline de inferencia funciona correctamente\n",
        "antes de procesar el conjunto completo.\n"
      ],
      "metadata": {
        "id": "6_n9Gf2L9j9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y_true, w = test_gen[0]\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "pred_softmax = y_pred[0]      # (512,512,3)\n",
        "pred_class = pred_softmax.argmax(axis=-1)   # (512,512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ltzi1-pUXZLu",
        "outputId": "4b9d88f8-df59-42ad-928b-8b2e876f38ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generación y almacenamiento de predicciones por parche\n",
        "\n",
        "Una vez entrenado el modelo, se generan predicciones para cada parche del conjunto de prueba. Para cada imagen:\n",
        "\n",
        "1. Se lee el GeoTIFF original y se conserva su metadata geoespacial (CRS, transformada y dimensiones).\n",
        "2. Se normaliza la imagen de la misma forma que en el generador de datos.\n",
        "3. Se obtiene la predicción del modelo y se asigna a cada píxel la clase de mayor probabilidad (`argmax`).\n",
        "4. Se guarda la máscara de clases resultante como un nuevo GeoTIFF (uint8), reutilizando la metadata original, de modo que cada predicción mantiene la georreferenciación del parche de entrada.\n",
        "\n",
        "Estas salidas se almacenan en la carpeta `/content/predictions`.\n"
      ],
      "metadata": {
        "id": "Cgi6dg0E88JL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import rasterio\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "os.makedirs(\"/content/predictions\", exist_ok=True)\n",
        "\n",
        "for i, img_path in enumerate(test_img_paths):\n",
        "\n",
        "    # --- Leer imagen original ---\n",
        "    with rasterio.open(img_path) as src:\n",
        "        meta = src.meta.copy()\n",
        "        img = src.read()               # (C,H,W)\n",
        "        img = np.transpose(img, (1,2,0)).astype(np.float32)\n",
        "\n",
        "    # --- Forzar RGB ---\n",
        "    img = img[..., :3]\n",
        "\n",
        "    # --- Normalizar ---\n",
        "    img = (img - img.min()) / (img.max() - img.min() + 1e-6)\n",
        "\n",
        "    # --- Expandir batch ---\n",
        "    X = np.expand_dims(img, axis=0)\n",
        "\n",
        "    # --- Predecir ---\n",
        "    pred_softmax = model.predict(X, verbose=0)[0]\n",
        "    pred_class = pred_softmax.argmax(axis=-1).astype(\"uint8\")  # (512,512)\n",
        "\n",
        "    # --- Guardar como GeoTIFF ---\n",
        "    meta.update({\n",
        "        \"count\": 1,\n",
        "        \"dtype\": \"uint8\"\n",
        "    })\n",
        "\n",
        "    out_path = f\"/content/predictions/pred_{i:05d}.tif\"\n",
        "\n",
        "    with rasterio.open(out_path, \"w\", **meta) as dst:\n",
        "        dst.write(pred_class, 1)\n",
        "\n",
        "print(\"Listo: todas las predicciones guardadas en /content/predictions\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TapuVorfXZIX",
        "outputId": "2b8e28f3-2aab-4b4b-8263-341e4e474bd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listo: todas las predicciones guardadas en /content/predictions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reconstrucción del mosaico georreferenciado de predicciones\n",
        "\n",
        "Finalmente, todas las máscaras de predicción individuales (un GeoTIFF por parche) se combinan en un único mosaico georreferenciado utilizando la función `merge` de `rasterio`. Este proceso:\n",
        "\n",
        "1. Abre cada archivo de predicción generado anteriormente.\n",
        "2. Combina los parches en una sola matriz raster, calculando la transformada espacial correspondiente.\n",
        "3. Actualiza la metadata (tamaño, transformada, tipo de dato) y escribe el resultado en un único archivo `pred_ccpp_test.tif`.\n",
        "\n",
        "El mosaico resultante permite visualizar en un solo GeoTIFF la delimitación completa de las clases (fondo, urbano, rural) sobre toda el área cubierta por el conjunto de prueba.\n"
      ],
      "metadata": {
        "id": "y3nXhlQZYWZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import rasterio\n",
        "from rasterio.merge import merge\n",
        "import glob\n",
        "import os"
      ],
      "metadata": {
        "id": "VkJLSbimYYTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PRED_DIR = \"/content/predictions\"   # Ajustar si es necesario\n",
        "pred_tiles = sorted(glob.glob(os.path.join(PRED_DIR, \"*.tif\")))\n",
        "len(pred_tiles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkAPmeNlYYQI",
        "outputId": "78c2706d-07f1-402c-d911-bee589d6a66e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "479"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_files = [rasterio.open(p) for p in pred_tiles]"
      ],
      "metadata": {
        "id": "ZMl2RHCkYTym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mosaic, out_transform = merge(src_files)"
      ],
      "metadata": {
        "id": "xiPoO96IYTvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_meta = src_files[0].meta.copy()\n",
        "out_meta.update({\n",
        "    \"height\": mosaic.shape[1],\n",
        "    \"width\": mosaic.shape[2],\n",
        "    \"transform\": out_transform,\n",
        "    \"count\": 1,\n",
        "    \"dtype\": \"uint8\"\n",
        "})"
      ],
      "metadata": {
        "id": "MYw0bP2XYTru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_final = \"/content/pred_ccpp_test.tif\"\n",
        "\n",
        "with rasterio.open(out_final, \"w\", **out_meta) as dst:\n",
        "    dst.write(mosaic)"
      ],
      "metadata": {
        "id": "YnTeXC4vXZFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for src in src_files:\n",
        "    src.close()"
      ],
      "metadata": {
        "id": "naw3mi1-Fvmq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}