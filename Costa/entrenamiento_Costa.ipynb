{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Experimento General"
      ],
      "metadata": {
        "id": "QrcXn-XGma8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_strict_conv_algorithm_picker=false\"\n",
        "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\""
      ],
      "metadata": {
        "id": "jo9PaC3kuVeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23TOba33L4qf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d08508c-392a-441c-ddc0-fcae3d93c2ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov 21 00:07:53 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0             51W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rasterio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1fR_3eaF9Wg",
        "outputId": "f153a50d-4d36-4499-f8c2-6f9ec4e542ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rasterio\n",
            "  Downloading rasterio-1.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting affine (from rasterio)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.12/dist-packages (from rasterio) (25.4.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from rasterio) (2025.10.5)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.12/dist-packages (from rasterio) (8.3.0)\n",
            "Collecting cligj>=0.5 (from rasterio)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.12/dist-packages (from rasterio) (2.0.2)\n",
            "Collecting click-plugins (from rasterio)\n",
            "  Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from rasterio) (3.2.5)\n",
            "Downloading rasterio-1.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m124.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: cligj, click-plugins, affine, rasterio\n",
            "Successfully installed affine-2.4.0 click-plugins-1.1.1.2 cligj-0.7.2 rasterio-1.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U segmentation-models==1.0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgH4TxgjKkZ-",
        "outputId": "c1c2ef33-1e97-48ad-bab5-84a1e38d837d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting segmentation-models==1.0.1\n",
            "  Downloading segmentation_models-1.0.1-py3-none-any.whl.metadata (938 bytes)\n",
            "Collecting keras-applications<=1.0.8,>=1.0.7 (from segmentation-models==1.0.1)\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting image-classifiers==1.0.0 (from segmentation-models==1.0.1)\n",
            "  Downloading image_classifiers-1.0.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting efficientnet==1.0.0 (from segmentation-models==1.0.1)\n",
            "  Downloading efficientnet-1.0.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from efficientnet==1.0.0->segmentation-models==1.0.1) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.12/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models==1.0.1) (2.0.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models==1.0.1) (3.15.1)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (1.16.3)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (2.37.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (2025.10.16)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (0.4)\n",
            "Downloading segmentation_models-1.0.1-py3-none-any.whl (33 kB)\n",
            "Downloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\n",
            "Downloading image_classifiers-1.0.0-py3-none-any.whl (19 kB)\n",
            "Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras-applications, image-classifiers, efficientnet, segmentation-models\n",
            "Successfully installed efficientnet-1.0.0 image-classifiers-1.0.0 keras-applications-1.0.8 segmentation-models-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
        "os.environ[\"SM_BACKEND\"] = \"tensorflow\""
      ],
      "metadata": {
        "id": "4MM_LZbiLLUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import segmentation_models as sm\n",
        "sm.set_framework('tf.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4F3_VXXOLM4v",
        "outputId": "700f67c6-cd40-4576-eb64-75f082c2b769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Models: using `tf.keras` framework.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rtBRRkA6hip9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a18e2e6-b0f9-4118-9fa5-ae318a9bc16d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import rasterio\n",
        "import cv2\n",
        "\n",
        "# ====== DESCOMPRIMIR ZIP EN COLAB (ajusta el nombre si es distinto) ======\n",
        "!unzip -q \"/content/drive/MyDrive/Proyecto Integrador/dataset_final.zip\" -d \"/content\"\n",
        "\n",
        "DATASET_ROOT = \"/content/dataset_final\"  # carpeta que contiene train/valid/test\n",
        "\n",
        "print(\"Subcarpetas:\", os.listdir(DATASET_ROOT))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTAS1VsZDkxM",
        "outputId": "72fa7c4d-eb2c-4a7a-84e4-0f45026224dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subcarpetas: ['val', 'test', 'train']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_mask_paths(split):\n",
        "    img_dir = os.path.join(DATASET_ROOT, split, \"images\")\n",
        "    mask_dir = os.path.join(DATASET_ROOT, split, \"masks\")\n",
        "\n",
        "    img_paths = sorted(glob.glob(os.path.join(img_dir, \"*.tif\")))\n",
        "    mask_paths = []\n",
        "\n",
        "    for p in img_paths:\n",
        "        fname = os.path.basename(p)  # img_000001.tif\n",
        "        mask_name = fname.replace(\"img_\", \"mask_\")\n",
        "        mask_paths.append(os.path.join(mask_dir, mask_name))\n",
        "\n",
        "    print(f\"{split}: {len(img_paths)} imágenes\")\n",
        "    return img_paths, mask_paths\n",
        "\n",
        "train_img_paths, train_mask_paths = get_image_mask_paths(\"train\")\n",
        "val_img_paths,   val_mask_paths   = get_image_mask_paths(\"val\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUTse2c6FuWQ",
        "outputId": "910d2070-97a6-448e-f1da-92945323992d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: 2467 imágenes\n",
            "val: 803 imágenes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clase Urbana - Rural"
      ],
      "metadata": {
        "id": "NCinjYltrVfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TIFFDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, image_paths, mask_paths,\n",
        "                 batch_size=4,\n",
        "                 shuffle=True,\n",
        "                 normalize=True,\n",
        "                 target_size=(512, 512),\n",
        "                 n_channels=3,      # <<< RGB\n",
        "                 n_classes=3,\n",
        "                 ignore_index=255):\n",
        "\n",
        "        self.image_paths = image_paths\n",
        "        self.mask_paths = mask_paths\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.normalize = normalize\n",
        "        self.target_size = target_size\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.ignore_index = ignore_index\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.image_paths) / self.batch_size)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indexes = np.arange(len(self.image_paths))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_indexes = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]\n",
        "        batch_imgs, batch_masks, batch_weights = [], [], []\n",
        "\n",
        "        for i in batch_indexes:\n",
        "\n",
        "            # ---------- Leer imagen ----------\n",
        "            with rasterio.open(self.image_paths[i]) as src:\n",
        "                img = src.read()               # (C, H, W)\n",
        "                img = np.transpose(img, (1, 2, 0))  # (H, W, C)\n",
        "\n",
        "            # ---------- Leer máscara ----------\n",
        "            with rasterio.open(self.mask_paths[i]) as src:\n",
        "                mask = src.read(1)\n",
        "\n",
        "            img = img.astype(np.float32)\n",
        "            mask = mask.astype(np.int32)\n",
        "\n",
        "            # ---------- Ajustar a 3 canales RGB ----------\n",
        "            if img.shape[-1] > 3:\n",
        "                img = img[..., :3]\n",
        "            elif img.shape[-1] < 3:\n",
        "                while img.shape[-1] < 3:\n",
        "                    img = np.concatenate([img, img[..., -1:]], axis=-1)\n",
        "\n",
        "            # ---------- Redimensionar ----------\n",
        "            if img.shape[0:2] != self.target_size:\n",
        "                img = cv2.resize(img, self.target_size, interpolation=cv2.INTER_LINEAR)\n",
        "            if mask.shape[0:2] != self.target_size:\n",
        "                mask = cv2.resize(mask, self.target_size, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "            # ---------- Normalización ----------\n",
        "            if self.normalize:\n",
        "                img_min, img_max = img.min(), img.max()\n",
        "                if img_max > img_min:\n",
        "                    img = (img - img_min) / (img_max - img_min)\n",
        "                else:\n",
        "                    img = np.zeros_like(img)\n",
        "\n",
        "            # ---------- sample_weights ----------\n",
        "            valid_mask = (mask != self.ignore_index).astype(\"float32\")\n",
        "\n",
        "            # ---------- Limitar mask a [0,1,2] ----------\n",
        "            mask_clipped = np.clip(mask, 0, self.n_classes - 1)\n",
        "\n",
        "            # ---------- One hot ----------\n",
        "            one_hot = np.eye(self.n_classes, dtype=\"float32\")[mask_clipped]\n",
        "\n",
        "            batch_imgs.append(img)\n",
        "            batch_masks.append(one_hot)\n",
        "            batch_weights.append(valid_mask[..., None])\n",
        "\n",
        "        X = np.stack(batch_imgs, axis=0)\n",
        "        y = np.stack(batch_masks, axis=0)\n",
        "        w = np.stack(batch_weights, axis=0)\n",
        "\n",
        "        return X, y, w\n",
        "\n",
        "train_gen = TIFFDataGenerator(train_img_paths, train_mask_paths,\n",
        "                              batch_size=4, shuffle=True,\n",
        "                              n_channels=3, n_classes=3, ignore_index=255)\n",
        "\n",
        "val_gen = TIFFDataGenerator(val_img_paths, val_mask_paths,\n",
        "                            batch_size=4, shuffle=False,\n",
        "                            n_channels=3, n_classes=3, ignore_index=255)\n"
      ],
      "metadata": {
        "id": "Fsz6_-EyFuS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "BACKBONE = 'resnet34'     # igual que tu amigo\n",
        "N_CLASSES = 3             # background, urbano, rural\n",
        "\n",
        "# ------------------------------\n",
        "# UNet + ResNet34 + ImageNet\n",
        "# ------------------------------\n",
        "model = sm.Unet(\n",
        "    backbone_name=BACKBONE,\n",
        "    encoder_weights='imagenet',           # <<< PREENTRENADO RGB\n",
        "    classes=N_CLASSES,\n",
        "    activation='softmax',                 # multiclase\n",
        "    input_shape=(512, 512, 3)             # <<< RGB\n",
        ")"
      ],
      "metadata": {
        "id": "ydN1IsFsJrvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- Métricas personalizadas por clase --------------------\n",
        "import tensorflow as tf\n",
        "\n",
        "# Métrica IoU para una clase específica\n",
        "def iou_for_class(class_id):\n",
        "    def metric(y_true, y_pred):\n",
        "        y_true_c = tf.cast(tf.equal(tf.argmax(y_true, -1), class_id), tf.float32)\n",
        "        y_pred_c = tf.cast(tf.equal(tf.argmax(y_pred, -1), class_id), tf.float32)\n",
        "        inter = tf.reduce_sum(y_true_c * y_pred_c)\n",
        "        union = tf.reduce_sum(y_true_c) + tf.reduce_sum(y_pred_c) - inter + 1e-7\n",
        "        return inter / union\n",
        "    metric.__name__ = f'iou_class_{class_id}'\n",
        "    return metric\n",
        "\n",
        "# Métrica F1 para una clase\n",
        "def f1_for_class(class_id):\n",
        "    def metric(y_true, y_pred):\n",
        "        y_true_c = tf.cast(tf.equal(tf.argmax(y_true, -1), class_id), tf.float32)\n",
        "        y_pred_c = tf.cast(tf.equal(tf.argmax(y_pred, -1), class_id), tf.float32)\n",
        "        tp = tf.reduce_sum(y_true_c * y_pred_c)\n",
        "        fp = tf.reduce_sum((1 - y_true_c) * y_pred_c)\n",
        "        fn = tf.reduce_sum(y_true_c * (1 - y_pred_c))\n",
        "        precision = tp / (tp + fp + 1e-7)\n",
        "        recall    = tp / (tp + fn + 1e-7)\n",
        "        return 2 * precision * recall / (precision + recall + 1e-7)\n",
        "    metric.__name__ = f'f1_class_{class_id}'\n",
        "    return metric\n",
        "\n",
        "# -------------------- Pérdida y métricas globales --------------------\n",
        "loss = sm.losses.CategoricalFocalLoss() + sm.losses.JaccardLoss()\n",
        "\n",
        "metrics = [\n",
        "    sm.metrics.IOUScore(threshold=None, name=\"iou_score\"),\n",
        "    sm.metrics.FScore(threshold=None, name=\"f1_score\"),\n",
        "    iou_for_class(0),  # background\n",
        "    iou_for_class(1),  # urbano\n",
        "    iou_for_class(2),  # rural\n",
        "    f1_for_class(0),\n",
        "    f1_for_class(1),\n",
        "    f1_for_class(2),\n",
        "]"
      ],
      "metadata": {
        "id": "HFBiv9dxFuPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=Adam(1e-4),\n",
        "    loss=loss,\n",
        "    metrics=[\n",
        "        sm.metrics.IOUScore(threshold=None, name=\"iou_score\"),\n",
        "        sm.metrics.FScore(threshold=None, name=\"f1_score\"),\n",
        "\n",
        "        iou_for_class(0),\n",
        "        iou_for_class(1),\n",
        "        iou_for_class(2),\n",
        "\n",
        "        f1_for_class(0),\n",
        "        f1_for_class(1),\n",
        "        f1_for_class(2),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "vssf05CdKJfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/Proyecto Integrador/ccpp_checkpoints\"\n",
        "\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(\"logs\", exist_ok=True)\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_iou_score',\n",
        "    mode='max',\n",
        "    patience=15,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "checkpoint_best = ModelCheckpoint(\n",
        "    filepath=os.path.join(CHECKPOINT_DIR, \"best_unet_multiclass.keras\"),\n",
        "    monitor='val_iou_score',\n",
        "    mode='max',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "checkpoint_every = ModelCheckpoint(\n",
        "    filepath=\"checkpoints/ckpt_epoch_{epoch:02d}.keras\",\n",
        "    save_best_only=False,\n",
        "    save_weights_only=False,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "tensorboard_cb = TensorBoard(\n",
        "    log_dir=\"logs/unet_multiclass\",\n",
        "    histogram_freq=0\n",
        ")\n"
      ],
      "metadata": {
        "id": "lkaXe6rJFuMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 60\n",
        "\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stop, checkpoint_best, checkpoint_every, tensorboard_cb]\n",
        ")\n",
        "\n",
        "print(\"Epochs realmente entrenadas:\", len(history.history['loss']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lljjhRrpDktZ",
        "outputId": "56932fa3-1693-4291-9c67-3a4130645404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - f1_class_0: 0.9071 - f1_class_1: 0.0559 - f1_class_2: 0.3150 - f1_score: 0.3148 - iou_class_0: 0.8546 - iou_class_1: 0.0426 - iou_class_2: 0.2084 - iou_score: 0.2503 - loss: 0.7703\n",
            "Epoch 1: val_iou_score improved from -inf to 0.30866, saving model to /content/drive/MyDrive/Proyecto Integrador/ccpp_checkpoints/best_unet_multiclass.keras\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 163ms/step - f1_class_0: 0.9072 - f1_class_1: 0.0560 - f1_class_2: 0.3152 - f1_score: 0.3149 - iou_class_0: 0.8547 - iou_class_1: 0.0426 - iou_class_2: 0.2086 - iou_score: 0.2505 - loss: 0.7701 - val_f1_class_0: 0.9661 - val_f1_class_1: 0.0000e+00 - val_f1_class_2: 0.0000e+00 - val_f1_score: 0.3226 - val_iou_class_0: 0.9439 - val_iou_class_1: 0.0000e+00 - val_iou_class_2: 0.0000e+00 - val_iou_score: 0.3087 - val_loss: 0.7105\n",
            "Epoch 2/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - f1_class_0: 0.9867 - f1_class_1: 0.1660 - f1_class_2: 0.5806 - f1_score: 0.5486 - iou_class_0: 0.9743 - iou_class_1: 0.1433 - iou_class_2: 0.4296 - iou_score: 0.4836 - loss: 0.5249\n",
            "Epoch 2: val_iou_score improved from 0.30866 to 0.47111, saving model to /content/drive/MyDrive/Proyecto Integrador/ccpp_checkpoints/best_unet_multiclass.keras\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 109ms/step - f1_class_0: 0.9867 - f1_class_1: 0.1660 - f1_class_2: 0.5806 - f1_score: 0.5486 - iou_class_0: 0.9743 - iou_class_1: 0.1434 - iou_class_2: 0.4297 - iou_score: 0.4837 - loss: 0.5249 - val_f1_class_0: 0.9813 - val_f1_class_1: 0.1362 - val_f1_class_2: 0.5011 - val_f1_score: 0.5301 - val_iou_class_0: 0.9655 - val_iou_class_1: 0.1131 - val_iou_class_2: 0.3646 - val_iou_score: 0.4711 - val_loss: 0.5421\n",
            "Epoch 3/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - f1_class_0: 0.9901 - f1_class_1: 0.1889 - f1_class_2: 0.6918 - f1_score: 0.6147 - iou_class_0: 0.9810 - iou_class_1: 0.1659 - iou_class_2: 0.5417 - iou_score: 0.5520 - loss: 0.4565\n",
            "Epoch 3: val_iou_score improved from 0.47111 to 0.48836, saving model to /content/drive/MyDrive/Proyecto Integrador/ccpp_checkpoints/best_unet_multiclass.keras\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 107ms/step - f1_class_0: 0.9901 - f1_class_1: 0.1890 - f1_class_2: 0.6918 - f1_score: 0.6147 - iou_class_0: 0.9810 - iou_class_1: 0.1659 - iou_class_2: 0.5417 - iou_score: 0.5520 - loss: 0.4565 - val_f1_class_0: 0.9872 - val_f1_class_1: 0.1373 - val_f1_class_2: 0.5265 - val_f1_score: 0.5471 - val_iou_class_0: 0.9756 - val_iou_class_1: 0.1174 - val_iou_class_2: 0.3834 - val_iou_score: 0.4884 - val_loss: 0.5221\n",
            "Epoch 4/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - f1_class_0: 0.9932 - f1_class_1: 0.2341 - f1_class_2: 0.7405 - f1_score: 0.6513 - iou_class_0: 0.9866 - iou_class_1: 0.2118 - iou_class_2: 0.5985 - iou_score: 0.5929 - loss: 0.4134\n",
            "Epoch 4: val_iou_score did not improve from 0.48836\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 107ms/step - f1_class_0: 0.9932 - f1_class_1: 0.2340 - f1_class_2: 0.7405 - f1_score: 0.6513 - iou_class_0: 0.9866 - iou_class_1: 0.2118 - iou_class_2: 0.5986 - iou_score: 0.5929 - loss: 0.4134 - val_f1_class_0: 0.9859 - val_f1_class_1: 0.1302 - val_f1_class_2: 0.4913 - val_f1_score: 0.5342 - val_iou_class_0: 0.9735 - val_iou_class_1: 0.1105 - val_iou_class_2: 0.3590 - val_iou_score: 0.4791 - val_loss: 0.5338\n",
            "Epoch 5/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - f1_class_0: 0.9952 - f1_class_1: 0.2461 - f1_class_2: 0.7732 - f1_score: 0.6682 - iou_class_0: 0.9906 - iou_class_1: 0.2270 - iou_class_2: 0.6423 - iou_score: 0.6155 - loss: 0.3893\n",
            "Epoch 5: val_iou_score did not improve from 0.48836\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 106ms/step - f1_class_0: 0.9952 - f1_class_1: 0.2461 - f1_class_2: 0.7732 - f1_score: 0.6682 - iou_class_0: 0.9906 - iou_class_1: 0.2271 - iou_class_2: 0.6423 - iou_score: 0.6155 - loss: 0.3893 - val_f1_class_0: 0.9870 - val_f1_class_1: 0.1335 - val_f1_class_2: 0.4689 - val_f1_score: 0.5288 - val_iou_class_0: 0.9755 - val_iou_class_1: 0.1120 - val_iou_class_2: 0.3392 - val_iou_score: 0.4744 - val_loss: 0.5401\n",
            "Epoch 6/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - f1_class_0: 0.9956 - f1_class_1: 0.2236 - f1_class_2: 0.7974 - f1_score: 0.6704 - iou_class_0: 0.9913 - iou_class_1: 0.2073 - iou_class_2: 0.6730 - iou_score: 0.6212 - loss: 0.3832\n",
            "Epoch 6: val_iou_score did not improve from 0.48836\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 104ms/step - f1_class_0: 0.9956 - f1_class_1: 0.2236 - f1_class_2: 0.7975 - f1_score: 0.6704 - iou_class_0: 0.9913 - iou_class_1: 0.2074 - iou_class_2: 0.6730 - iou_score: 0.6213 - loss: 0.3832 - val_f1_class_0: 0.9847 - val_f1_class_1: 0.1305 - val_f1_class_2: 0.4822 - val_f1_score: 0.5318 - val_iou_class_0: 0.9714 - val_iou_class_1: 0.1088 - val_iou_class_2: 0.3535 - val_iou_score: 0.4771 - val_loss: 0.5386\n",
            "Epoch 7/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - f1_class_0: 0.9955 - f1_class_1: 0.2669 - f1_class_2: 0.8168 - f1_score: 0.6914 - iou_class_0: 0.9911 - iou_class_1: 0.2477 - iou_class_2: 0.6995 - iou_score: 0.6436 - loss: 0.3612\n",
            "Epoch 7: val_iou_score did not improve from 0.48836\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 105ms/step - f1_class_0: 0.9955 - f1_class_1: 0.2669 - f1_class_2: 0.8168 - f1_score: 0.6913 - iou_class_0: 0.9911 - iou_class_1: 0.2476 - iou_class_2: 0.6995 - iou_score: 0.6436 - loss: 0.3612 - val_f1_class_0: 0.9879 - val_f1_class_1: 0.1238 - val_f1_class_2: 0.5105 - val_f1_score: 0.5401 - val_iou_class_0: 0.9770 - val_iou_class_1: 0.1059 - val_iou_class_2: 0.3793 - val_iou_score: 0.4867 - val_loss: 0.5267\n",
            "Epoch 8/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - f1_class_0: 0.9966 - f1_class_1: 0.2551 - f1_class_2: 0.8376 - f1_score: 0.6949 - iou_class_0: 0.9932 - iou_class_1: 0.2393 - iou_class_2: 0.7254 - iou_score: 0.6504 - loss: 0.3530\n",
            "Epoch 8: val_iou_score did not improve from 0.48836\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 109ms/step - f1_class_0: 0.9966 - f1_class_1: 0.2551 - f1_class_2: 0.8376 - f1_score: 0.6949 - iou_class_0: 0.9932 - iou_class_1: 0.2393 - iou_class_2: 0.7254 - iou_score: 0.6504 - loss: 0.3530 - val_f1_class_0: 0.9852 - val_f1_class_1: 0.1099 - val_f1_class_2: 0.4613 - val_f1_score: 0.5184 - val_iou_class_0: 0.9724 - val_iou_class_1: 0.0931 - val_iou_class_2: 0.3357 - val_iou_score: 0.4666 - val_loss: 0.5510\n",
            "Epoch 9/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - f1_class_0: 0.9952 - f1_class_1: 0.2497 - f1_class_2: 0.8343 - f1_score: 0.6920 - iou_class_0: 0.9911 - iou_class_1: 0.2305 - iou_class_2: 0.7242 - iou_score: 0.6469 - loss: 0.3580\n",
            "Epoch 9: val_iou_score improved from 0.48836 to 0.49308, saving model to /content/drive/MyDrive/Proyecto Integrador/ccpp_checkpoints/best_unet_multiclass.keras\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 106ms/step - f1_class_0: 0.9952 - f1_class_1: 0.2497 - f1_class_2: 0.8343 - f1_score: 0.6920 - iou_class_0: 0.9911 - iou_class_1: 0.2305 - iou_class_2: 0.7242 - iou_score: 0.6470 - loss: 0.3579 - val_f1_class_0: 0.9875 - val_f1_class_1: 0.1253 - val_f1_class_2: 0.5304 - val_f1_score: 0.5473 - val_iou_class_0: 0.9764 - val_iou_class_1: 0.1080 - val_iou_class_2: 0.3963 - val_iou_score: 0.4931 - val_loss: 0.5220\n",
            "Epoch 10/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - f1_class_0: 0.9970 - f1_class_1: 0.2530 - f1_class_2: 0.8600 - f1_score: 0.7024 - iou_class_0: 0.9941 - iou_class_1: 0.2403 - iou_class_2: 0.7596 - iou_score: 0.6631 - loss: 0.3400\n",
            "Epoch 10: val_iou_score did not improve from 0.49308\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 107ms/step - f1_class_0: 0.9970 - f1_class_1: 0.2530 - f1_class_2: 0.8600 - f1_score: 0.7024 - iou_class_0: 0.9941 - iou_class_1: 0.2403 - iou_class_2: 0.7595 - iou_score: 0.6631 - loss: 0.3400 - val_f1_class_0: 0.9858 - val_f1_class_1: 0.1383 - val_f1_class_2: 0.4830 - val_f1_score: 0.5353 - val_iou_class_0: 0.9733 - val_iou_class_1: 0.1165 - val_iou_class_2: 0.3567 - val_iou_score: 0.4818 - val_loss: 0.5357\n",
            "Epoch 11/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - f1_class_0: 0.9962 - f1_class_1: 0.2615 - f1_class_2: 0.8392 - f1_score: 0.6981 - iou_class_0: 0.9925 - iou_class_1: 0.2459 - iou_class_2: 0.7321 - iou_score: 0.6556 - loss: 0.3489\n",
            "Epoch 11: val_iou_score improved from 0.49308 to 0.49587, saving model to /content/drive/MyDrive/Proyecto Integrador/ccpp_checkpoints/best_unet_multiclass.keras\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 116ms/step - f1_class_0: 0.9962 - f1_class_1: 0.2615 - f1_class_2: 0.8392 - f1_score: 0.6981 - iou_class_0: 0.9925 - iou_class_1: 0.2460 - iou_class_2: 0.7321 - iou_score: 0.6556 - loss: 0.3489 - val_f1_class_0: 0.9884 - val_f1_class_1: 0.1334 - val_f1_class_2: 0.5301 - val_f1_score: 0.5503 - val_iou_class_0: 0.9780 - val_iou_class_1: 0.1135 - val_iou_class_2: 0.3972 - val_iou_score: 0.4959 - val_loss: 0.5193\n",
            "Epoch 12/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - f1_class_0: 0.9973 - f1_class_1: 0.2507 - f1_class_2: 0.8672 - f1_score: 0.7041 - iou_class_0: 0.9947 - iou_class_1: 0.2368 - iou_class_2: 0.7707 - iou_score: 0.6660 - loss: 0.3368\n",
            "Epoch 12: val_iou_score did not improve from 0.49587\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 107ms/step - f1_class_0: 0.9973 - f1_class_1: 0.2508 - f1_class_2: 0.8672 - f1_score: 0.7041 - iou_class_0: 0.9947 - iou_class_1: 0.2368 - iou_class_2: 0.7708 - iou_score: 0.6660 - loss: 0.3368 - val_f1_class_0: 0.9882 - val_f1_class_1: 0.1293 - val_f1_class_2: 0.4945 - val_f1_score: 0.5370 - val_iou_class_0: 0.9777 - val_iou_class_1: 0.1098 - val_iou_class_2: 0.3637 - val_iou_score: 0.4833 - val_loss: 0.5323\n",
            "Epoch 13/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - f1_class_0: 0.9972 - f1_class_1: 0.2665 - f1_class_2: 0.8743 - f1_score: 0.7119 - iou_class_0: 0.9945 - iou_class_1: 0.2514 - iou_class_2: 0.7804 - iou_score: 0.6743 - loss: 0.3287\n",
            "Epoch 13: val_iou_score did not improve from 0.49587\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 105ms/step - f1_class_0: 0.9972 - f1_class_1: 0.2665 - f1_class_2: 0.8743 - f1_score: 0.7120 - iou_class_0: 0.9945 - iou_class_1: 0.2514 - iou_class_2: 0.7804 - iou_score: 0.6743 - loss: 0.3287 - val_f1_class_0: 0.9880 - val_f1_class_1: 0.1112 - val_f1_class_2: 0.4347 - val_f1_score: 0.5112 - val_iou_class_0: 0.9773 - val_iou_class_1: 0.0962 - val_iou_class_2: 0.3160 - val_iou_score: 0.4630 - val_loss: 0.5549\n",
            "Epoch 14/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - f1_class_0: 0.9977 - f1_class_1: 0.2795 - f1_class_2: 0.8850 - f1_score: 0.7201 - iou_class_0: 0.9954 - iou_class_1: 0.2684 - iou_class_2: 0.7987 - iou_score: 0.6865 - loss: 0.3159\n",
            "Epoch 14: val_iou_score did not improve from 0.49587\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 115ms/step - f1_class_0: 0.9977 - f1_class_1: 0.2795 - f1_class_2: 0.8850 - f1_score: 0.7201 - iou_class_0: 0.9954 - iou_class_1: 0.2685 - iou_class_2: 0.7987 - iou_score: 0.6865 - loss: 0.3159 - val_f1_class_0: 0.9874 - val_f1_class_1: 0.1328 - val_f1_class_2: 0.4222 - val_f1_score: 0.5140 - val_iou_class_0: 0.9763 - val_iou_class_1: 0.1117 - val_iou_class_2: 0.3045 - val_iou_score: 0.4639 - val_loss: 0.5555\n",
            "Epoch 15/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - f1_class_0: 0.9964 - f1_class_1: 0.2568 - f1_class_2: 0.8733 - f1_score: 0.7081 - iou_class_0: 0.9930 - iou_class_1: 0.2390 - iou_class_2: 0.7789 - iou_score: 0.6691 - loss: 0.3348\n",
            "Epoch 15: val_iou_score did not improve from 0.49587\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 107ms/step - f1_class_0: 0.9964 - f1_class_1: 0.2568 - f1_class_2: 0.8733 - f1_score: 0.7081 - iou_class_0: 0.9930 - iou_class_1: 0.2390 - iou_class_2: 0.7789 - iou_score: 0.6692 - loss: 0.3348 - val_f1_class_0: 0.9882 - val_f1_class_1: 0.1421 - val_f1_class_2: 0.4757 - val_f1_score: 0.5350 - val_iou_class_0: 0.9777 - val_iou_class_1: 0.1203 - val_iou_class_2: 0.3519 - val_iou_score: 0.4829 - val_loss: 0.5348\n",
            "Epoch 16/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - f1_class_0: 0.9978 - f1_class_1: 0.2765 - f1_class_2: 0.8872 - f1_score: 0.7197 - iou_class_0: 0.9956 - iou_class_1: 0.2665 - iou_class_2: 0.8011 - iou_score: 0.6865 - loss: 0.3159\n",
            "Epoch 16: val_iou_score did not improve from 0.49587\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 112ms/step - f1_class_0: 0.9978 - f1_class_1: 0.2765 - f1_class_2: 0.8871 - f1_score: 0.7197 - iou_class_0: 0.9956 - iou_class_1: 0.2665 - iou_class_2: 0.8011 - iou_score: 0.6865 - loss: 0.3159 - val_f1_class_0: 0.9872 - val_f1_class_1: 0.1230 - val_f1_class_2: 0.5035 - val_f1_score: 0.5376 - val_iou_class_0: 0.9756 - val_iou_class_1: 0.1050 - val_iou_class_2: 0.3648 - val_iou_score: 0.4816 - val_loss: 0.5379\n",
            "Epoch 17/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - f1_class_0: 0.9974 - f1_class_1: 0.2969 - f1_class_2: 0.8824 - f1_score: 0.7250 - iou_class_0: 0.9949 - iou_class_1: 0.2840 - iou_class_2: 0.7945 - iou_score: 0.6902 - loss: 0.3127\n",
            "Epoch 17: val_iou_score did not improve from 0.49587\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 110ms/step - f1_class_0: 0.9974 - f1_class_1: 0.2969 - f1_class_2: 0.8824 - f1_score: 0.7250 - iou_class_0: 0.9949 - iou_class_1: 0.2840 - iou_class_2: 0.7945 - iou_score: 0.6902 - loss: 0.3127 - val_f1_class_0: 0.9886 - val_f1_class_1: 0.1193 - val_f1_class_2: 0.5054 - val_f1_score: 0.5375 - val_iou_class_0: 0.9784 - val_iou_class_1: 0.1037 - val_iou_class_2: 0.3765 - val_iou_score: 0.4858 - val_loss: 0.5318\n",
            "Epoch 18/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - f1_class_0: 0.9982 - f1_class_1: 0.3109 - f1_class_2: 0.9033 - f1_score: 0.7369 - iou_class_0: 0.9964 - iou_class_1: 0.3008 - iou_class_2: 0.8266 - iou_score: 0.7070 - loss: 0.2949\n",
            "Epoch 18: val_iou_score did not improve from 0.49587\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 106ms/step - f1_class_0: 0.9982 - f1_class_1: 0.3108 - f1_class_2: 0.9033 - f1_score: 0.7369 - iou_class_0: 0.9964 - iou_class_1: 0.3007 - iou_class_2: 0.8266 - iou_score: 0.7070 - loss: 0.2949 - val_f1_class_0: 0.9882 - val_f1_class_1: 0.1231 - val_f1_class_2: 0.5286 - val_f1_score: 0.5464 - val_iou_class_0: 0.9777 - val_iou_class_1: 0.1057 - val_iou_class_2: 0.3937 - val_iou_score: 0.4921 - val_loss: 0.5255\n",
            "Epoch 19/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - f1_class_0: 0.9974 - f1_class_1: 0.3148 - f1_class_2: 0.8942 - f1_score: 0.7349 - iou_class_0: 0.9949 - iou_class_1: 0.3030 - iou_class_2: 0.8120 - iou_score: 0.7024 - loss: 0.3003\n",
            "Epoch 19: val_iou_score did not improve from 0.49587\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 110ms/step - f1_class_0: 0.9974 - f1_class_1: 0.3147 - f1_class_2: 0.8942 - f1_score: 0.7349 - iou_class_0: 0.9949 - iou_class_1: 0.3030 - iou_class_2: 0.8120 - iou_score: 0.7024 - loss: 0.3003 - val_f1_class_0: 0.9887 - val_f1_class_1: 0.1337 - val_f1_class_2: 0.4918 - val_f1_score: 0.5379 - val_iou_class_0: 0.9785 - val_iou_class_1: 0.1144 - val_iou_class_2: 0.3722 - val_iou_score: 0.4881 - val_loss: 0.5288\n",
            "Epoch 20/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - f1_class_0: 0.9981 - f1_class_1: 0.3064 - f1_class_2: 0.9086 - f1_score: 0.7373 - iou_class_0: 0.9963 - iou_class_1: 0.2977 - iou_class_2: 0.8353 - iou_score: 0.7090 - loss: 0.2932\n",
            "Epoch 20: val_iou_score did not improve from 0.49587\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 122ms/step - f1_class_0: 0.9981 - f1_class_1: 0.3064 - f1_class_2: 0.9086 - f1_score: 0.7373 - iou_class_0: 0.9963 - iou_class_1: 0.2976 - iou_class_2: 0.8353 - iou_score: 0.7089 - loss: 0.2932 - val_f1_class_0: 0.9887 - val_f1_class_1: 0.1293 - val_f1_class_2: 0.5015 - val_f1_score: 0.5397 - val_iou_class_0: 0.9786 - val_iou_class_1: 0.1112 - val_iou_class_2: 0.3720 - val_iou_score: 0.4871 - val_loss: 0.5314\n",
            "Epoch 21/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - f1_class_0: 0.9984 - f1_class_1: 0.2736 - f1_class_2: 0.9087 - f1_score: 0.7265 - iou_class_0: 0.9968 - iou_class_1: 0.2655 - iou_class_2: 0.8361 - iou_score: 0.6987 - loss: 0.3030\n",
            "Epoch 21: val_iou_score did not improve from 0.49587\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 109ms/step - f1_class_0: 0.9984 - f1_class_1: 0.2736 - f1_class_2: 0.9087 - f1_score: 0.7265 - iou_class_0: 0.9968 - iou_class_1: 0.2655 - iou_class_2: 0.8361 - iou_score: 0.6987 - loss: 0.3030 - val_f1_class_0: 0.9886 - val_f1_class_1: 0.1088 - val_f1_class_2: 0.4316 - val_f1_score: 0.5095 - val_iou_class_0: 0.9784 - val_iou_class_1: 0.0959 - val_iou_class_2: 0.3061 - val_iou_score: 0.4600 - val_loss: 0.5615\n",
            "Epoch 22/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - f1_class_0: 0.9982 - f1_class_1: 0.2868 - f1_class_2: 0.9166 - f1_score: 0.7334 - iou_class_0: 0.9966 - iou_class_1: 0.2795 - iou_class_2: 0.8483 - iou_score: 0.7074 - loss: 0.2946\n",
            "Epoch 22: val_iou_score did not improve from 0.49587\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 107ms/step - f1_class_0: 0.9982 - f1_class_1: 0.2868 - f1_class_2: 0.9165 - f1_score: 0.7334 - iou_class_0: 0.9965 - iou_class_1: 0.2795 - iou_class_2: 0.8483 - iou_score: 0.7074 - loss: 0.2946 - val_f1_class_0: 0.9871 - val_f1_class_1: 0.1203 - val_f1_class_2: 0.3922 - val_f1_score: 0.4997 - val_iou_class_0: 0.9758 - val_iou_class_1: 0.1022 - val_iou_class_2: 0.2778 - val_iou_score: 0.4518 - val_loss: 0.5676\n",
            "Epoch 23/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - f1_class_0: 0.9972 - f1_class_1: 0.2933 - f1_class_2: 0.8857 - f1_score: 0.7248 - iou_class_0: 0.9945 - iou_class_1: 0.2811 - iou_class_2: 0.8030 - iou_score: 0.6919 - loss: 0.3109\n",
            "Epoch 23: val_iou_score did not improve from 0.49587\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 106ms/step - f1_class_0: 0.9972 - f1_class_1: 0.2933 - f1_class_2: 0.8857 - f1_score: 0.7248 - iou_class_0: 0.9945 - iou_class_1: 0.2811 - iou_class_2: 0.8031 - iou_score: 0.6920 - loss: 0.3109 - val_f1_class_0: 0.9892 - val_f1_class_1: 0.1396 - val_f1_class_2: 0.4867 - val_f1_score: 0.5383 - val_iou_class_0: 0.9795 - val_iou_class_1: 0.1197 - val_iou_class_2: 0.3580 - val_iou_score: 0.4855 - val_loss: 0.5319\n",
            "Epoch 24/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - f1_class_0: 0.9983 - f1_class_1: 0.2813 - f1_class_2: 0.9114 - f1_score: 0.7299 - iou_class_0: 0.9966 - iou_class_1: 0.2727 - iou_class_2: 0.8412 - iou_score: 0.7028 - loss: 0.2994\n",
            "Epoch 24: val_iou_score did not improve from 0.49587\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 106ms/step - f1_class_0: 0.9983 - f1_class_1: 0.2813 - f1_class_2: 0.9114 - f1_score: 0.7299 - iou_class_0: 0.9966 - iou_class_1: 0.2728 - iou_class_2: 0.8411 - iou_score: 0.7028 - loss: 0.2994 - val_f1_class_0: 0.9882 - val_f1_class_1: 0.1074 - val_f1_class_2: 0.4892 - val_f1_score: 0.5281 - val_iou_class_0: 0.9777 - val_iou_class_1: 0.0936 - val_iou_class_2: 0.3619 - val_iou_score: 0.4776 - val_loss: 0.5430\n",
            "Epoch 25/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - f1_class_0: 0.9983 - f1_class_1: 0.2709 - f1_class_2: 0.9141 - f1_score: 0.7273 - iou_class_0: 0.9965 - iou_class_1: 0.2631 - iou_class_2: 0.8443 - iou_score: 0.7006 - loss: 0.3012\n",
            "Epoch 25: val_iou_score did not improve from 0.49587\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 104ms/step - f1_class_0: 0.9983 - f1_class_1: 0.2709 - f1_class_2: 0.9141 - f1_score: 0.7273 - iou_class_0: 0.9965 - iou_class_1: 0.2631 - iou_class_2: 0.8443 - iou_score: 0.7006 - loss: 0.3012 - val_f1_class_0: 0.9889 - val_f1_class_1: 0.1264 - val_f1_class_2: 0.4831 - val_f1_score: 0.5327 - val_iou_class_0: 0.9790 - val_iou_class_1: 0.1100 - val_iou_class_2: 0.3565 - val_iou_score: 0.4817 - val_loss: 0.5364\n",
            "Epoch 26/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - f1_class_0: 0.9986 - f1_class_1: 0.3045 - f1_class_2: 0.9259 - f1_score: 0.7427 - iou_class_0: 0.9973 - iou_class_1: 0.2979 - iou_class_2: 0.8640 - iou_score: 0.7191 - loss: 0.2823\n",
            "Epoch 26: val_iou_score did not improve from 0.49587\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 108ms/step - f1_class_0: 0.9986 - f1_class_1: 0.3045 - f1_class_2: 0.9259 - f1_score: 0.7427 - iou_class_0: 0.9973 - iou_class_1: 0.2979 - iou_class_2: 0.8640 - iou_score: 0.7191 - loss: 0.2823 - val_f1_class_0: 0.9884 - val_f1_class_1: 0.1280 - val_f1_class_2: 0.5166 - val_f1_score: 0.5442 - val_iou_class_0: 0.9781 - val_iou_class_1: 0.1100 - val_iou_class_2: 0.3818 - val_iou_score: 0.4898 - val_loss: 0.5297\n",
            "Epoch 26: early stopping\n",
            "Restoring model weights from the end of the best epoch: 11.\n",
            "Epochs realmente entrenadas: 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import segmentation_models as sm\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
        "sm.set_framework('tf.keras')\n",
        "\n",
        "custom_objects = {\n",
        "    'CategoricalFocalJaccardLoss': sm.losses.CategoricalFocalJaccardLoss,\n",
        "    'iou_score': sm.metrics.IOUScore,\n",
        "    'f1_score': sm.metrics.FScore,\n",
        "    # métricas custom:\n",
        "    'iou_class_0': iou_for_class(0),\n",
        "    'iou_class_1': iou_for_class(1),\n",
        "    'iou_class_2': iou_for_class(2),\n",
        "    'f1_class_0':  f1_for_class(0),\n",
        "    'f1_class_1':  f1_for_class(1),\n",
        "    'f1_class_2':  f1_for_class(2),\n",
        "}\n",
        "\n",
        "ckpt_path = \"checkpoints/best_unet_multiclass.keras\"  # o uno de los ckpt_epoch_XX\n",
        "assert os.path.exists(ckpt_path), \"No existe el checkpoint elegido\"\n",
        "\n",
        "model = tf.keras.models.load_model(ckpt_path, custom_objects=custom_objects)\n",
        "print(\"Modelo cargado desde:\", ckpt_path)\n"
      ],
      "metadata": {
        "id": "7w296YrzDkqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history2 = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=20,   # epochs adicionales\n",
        "    callbacks=[early_stop, checkpoint_best, checkpoint_every, tensorboard_cb]\n",
        ")\n"
      ],
      "metadata": {
        "id": "4DETd65DDkbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test"
      ],
      "metadata": {
        "id": "Z7bzH8xGW5JK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "TEST_IMG_DIR = os.path.join(DATASET_ROOT, \"test/images\")\n",
        "TEST_MASK_DIR = os.path.join(DATASET_ROOT, \"test/masks\")\n",
        "\n",
        "test_img_paths = sorted(glob.glob(os.path.join(TEST_IMG_DIR, \"*.tif\")))\n",
        "test_mask_paths = sorted(glob.glob(os.path.join(TEST_MASK_DIR, \"*.tif\")))\n",
        "\n",
        "print(\"Test imágenes:\", len(test_img_paths))\n",
        "print(\"Test máscaras:\", len(test_mask_paths))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4yZPE0AFvwM",
        "outputId": "02c3b74d-b45e-46a0-bceb-93ec9149e74b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test imágenes: 479\n",
            "Test máscaras: 479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_gen = TIFFDataGenerator(\n",
        "    test_img_paths,\n",
        "    test_mask_paths,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    target_size=(512,512),\n",
        "    n_channels=3,\n",
        "    n_classes=3\n",
        ")"
      ],
      "metadata": {
        "id": "BY36wbCKFvs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics = model.evaluate(test_gen, verbose=1)\n",
        "for name, value in zip(model.metrics_names, test_metrics):\n",
        "    print(f\"{name}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZdX6TqpFvps",
        "outputId": "373a5a7f-e188-4ed3-ba46-6697e7ec6e74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step - f1_class_0: 0.9970 - f1_class_1: 0.0000e+00 - f1_class_2: 0.5994 - f1_score: 0.5316 - iou_class_0: 0.9940 - iou_class_1: 0.0000e+00 - iou_class_2: 0.4728 - iou_score: 0.4883 - loss: 0.5165\n",
            "loss: 0.5199595093727112\n",
            "compile_metrics: 0.48464134335517883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y_true, w = test_gen[0]\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "pred_softmax = y_pred[0]      # (512,512,3)\n",
        "pred_class = pred_softmax.argmax(axis=-1)   # (512,512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ltzi1-pUXZLu",
        "outputId": "4b9d88f8-df59-42ad-928b-8b2e876f38ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import rasterio\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "os.makedirs(\"/content/predictions\", exist_ok=True)\n",
        "\n",
        "for i, img_path in enumerate(test_img_paths):\n",
        "\n",
        "    # --- Leer imagen original ---\n",
        "    with rasterio.open(img_path) as src:\n",
        "        meta = src.meta.copy()\n",
        "        img = src.read()               # (C,H,W)\n",
        "        img = np.transpose(img, (1,2,0)).astype(np.float32)\n",
        "\n",
        "    # --- Forzar RGB ---\n",
        "    img = img[..., :3]\n",
        "\n",
        "    # --- Normalizar ---\n",
        "    img = (img - img.min()) / (img.max() - img.min() + 1e-6)\n",
        "\n",
        "    # --- Expandir batch ---\n",
        "    X = np.expand_dims(img, axis=0)\n",
        "\n",
        "    # --- Predecir ---\n",
        "    pred_softmax = model.predict(X, verbose=0)[0]\n",
        "    pred_class = pred_softmax.argmax(axis=-1).astype(\"uint8\")  # (512,512)\n",
        "\n",
        "    # --- Guardar como GeoTIFF ---\n",
        "    meta.update({\n",
        "        \"count\": 1,\n",
        "        \"dtype\": \"uint8\"\n",
        "    })\n",
        "\n",
        "    out_path = f\"/content/predictions/pred_{i:05d}.tif\"\n",
        "\n",
        "    with rasterio.open(out_path, \"w\", **meta) as dst:\n",
        "        dst.write(pred_class, 1)\n",
        "\n",
        "print(\"Listo: todas las predicciones guardadas en /content/predictions\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TapuVorfXZIX",
        "outputId": "2b8e28f3-2aab-4b4b-8263-341e4e474bd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listo: todas las predicciones guardadas en /content/predictions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparando el Entorno"
      ],
      "metadata": {
        "id": "y3nXhlQZYWZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import rasterio\n",
        "from rasterio.merge import merge\n",
        "import glob\n",
        "import os"
      ],
      "metadata": {
        "id": "VkJLSbimYYTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PRED_DIR = \"/content/predictions\"   # Ajustar si es necesario\n",
        "pred_tiles = sorted(glob.glob(os.path.join(PRED_DIR, \"*.tif\")))\n",
        "len(pred_tiles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkAPmeNlYYQI",
        "outputId": "78c2706d-07f1-402c-d911-bee589d6a66e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "479"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_files = [rasterio.open(p) for p in pred_tiles]"
      ],
      "metadata": {
        "id": "ZMl2RHCkYTym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mosaic, out_transform = merge(src_files)"
      ],
      "metadata": {
        "id": "xiPoO96IYTvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_meta = src_files[0].meta.copy()\n",
        "out_meta.update({\n",
        "    \"height\": mosaic.shape[1],\n",
        "    \"width\": mosaic.shape[2],\n",
        "    \"transform\": out_transform,\n",
        "    \"count\": 1,\n",
        "    \"dtype\": \"uint8\"\n",
        "})"
      ],
      "metadata": {
        "id": "MYw0bP2XYTru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_final = \"/content/pred_ccpp_test.tif\"\n",
        "\n",
        "with rasterio.open(out_final, \"w\", **out_meta) as dst:\n",
        "    dst.write(mosaic)"
      ],
      "metadata": {
        "id": "YnTeXC4vXZFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for src in src_files:\n",
        "    src.close()"
      ],
      "metadata": {
        "id": "naw3mi1-Fvmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sin Clase Urbana"
      ],
      "metadata": {
        "id": "m_gjLOCTmUPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ImageNet"
      ],
      "metadata": {
        "id": "2WWq-JKa8tCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TIFFDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, image_paths, mask_paths,\n",
        "                 batch_size=4,\n",
        "                 shuffle=True,\n",
        "                 normalize=True,\n",
        "                 target_size=(512, 512),\n",
        "                 n_channels=3,\n",
        "                 n_classes=2,       # SOLO dos clases: fondo / rural\n",
        "                 ignore_index=255):\n",
        "\n",
        "        self.image_paths = image_paths\n",
        "        self.mask_paths = mask_paths\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.normalize = normalize\n",
        "        self.target_size = target_size\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.ignore_index = ignore_index\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.image_paths) / self.batch_size)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indexes = np.arange(len(self.image_paths))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_indexes = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]\n",
        "        batch_imgs, batch_masks, batch_weights = [], [], []\n",
        "\n",
        "        for i in batch_indexes:\n",
        "\n",
        "            # ---------- Leer imagen ----------\n",
        "            with rasterio.open(self.image_paths[i]) as src:\n",
        "                img = src.read()\n",
        "                img = np.transpose(img, (1, 2, 0))\n",
        "\n",
        "            # ---------- Leer máscara ----------\n",
        "            with rasterio.open(self.mask_paths[i]) as src:\n",
        "                mask = src.read(1)\n",
        "\n",
        "            img = img.astype(np.float32)\n",
        "            mask = mask.astype(np.int32)\n",
        "\n",
        "            # ---------- Asegurar 3 canales RGB ----------\n",
        "            if img.shape[-1] > 3:\n",
        "                img = img[..., :3]\n",
        "            elif img.shape[-1] < 3:\n",
        "                while img.shape[-1] < 3:\n",
        "                    img = np.concatenate([img, img[..., -1:]], axis=-1)\n",
        "\n",
        "            # ---------- Redimensionar ----------\n",
        "            if img.shape[0:2] != self.target_size:\n",
        "                img = cv2.resize(img, self.target_size, interpolation=cv2.INTER_LINEAR)\n",
        "            if mask.shape[0:2] != self.target_size:\n",
        "                mask = cv2.resize(mask, self.target_size, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "            # ---------- Normalizar ----------\n",
        "            if self.normalize:\n",
        "                img_min, img_max = img.min(), img.max()\n",
        "                if img_max > img_min:\n",
        "                    img = (img - img_min) / (img_max - img_min)\n",
        "                else:\n",
        "                    img = np.zeros_like(img)\n",
        "\n",
        "            # ============================================================\n",
        "            # 🔥 REMAPEO PARA ENTRENAMIENTO SOLO DE CP RURAL\n",
        "            # ============================================================\n",
        "            # urbano (1) → 255 (ignore)\n",
        "            # rural  (2) → 1\n",
        "            # fondo  (0) → 0\n",
        "            mask = np.where(mask == 1, 255, mask)   # urbano → ignorar\n",
        "            mask = np.where(mask == 2, 1, mask)     # rural → 1\n",
        "\n",
        "            mask = mask.astype(np.int32)\n",
        "            mask = np.where((mask != 0) & (mask != 1) & (mask != 255), 255, mask)\n",
        "\n",
        "            # ---------- Pesos (ignorar urbano y píxeles negros) ----------\n",
        "            valid_mask = (mask != self.ignore_index).astype(\"float32\")\n",
        "\n",
        "            # ---------- One-hot ----------\n",
        "            mask_clipped = np.clip(mask, 0, self.n_classes - 1)\n",
        "            one_hot = np.eye(self.n_classes, dtype=\"float32\")[mask_clipped]\n",
        "\n",
        "            batch_imgs.append(img)\n",
        "            batch_masks.append(one_hot)\n",
        "            batch_weights.append(valid_mask[..., None])\n",
        "\n",
        "        X = np.stack(batch_imgs, axis=0)\n",
        "        y = np.stack(batch_masks, axis=0)\n",
        "        w = np.stack(batch_weights, axis=0)\n",
        "\n",
        "        return X, y, w"
      ],
      "metadata": {
        "id": "Dy-YBo47ZDJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen = TIFFDataGenerator(\n",
        "    train_img_paths, train_mask_paths,\n",
        "    batch_size=2, shuffle=True,\n",
        "    n_channels=3, n_classes=2, ignore_index=255\n",
        ")\n",
        "\n",
        "val_gen = TIFFDataGenerator(\n",
        "    val_img_paths, val_mask_paths,\n",
        "    batch_size=2, shuffle=False,\n",
        "    n_channels=3, n_classes=2, ignore_index=255\n",
        ")"
      ],
      "metadata": {
        "id": "1DAcnHC5sUpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BACKBONE = 'resnet34'\n",
        "N_CLASSES = 2\n",
        "\n",
        "model = sm.Unet(\n",
        "    backbone_name=BACKBONE,\n",
        "    encoder_weights='imagenet',\n",
        "    classes=N_CLASSES,\n",
        "    activation='softmax',\n",
        "    input_shape=(512, 512, 3)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kH9fYF93s42h",
        "outputId": "5c7d4e57-d849-427c-dfe2-e47e51d0d660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet34_imagenet_1000_no_top.h5\n",
            "\u001b[1m85521592/85521592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def iou_for_class(class_id):\n",
        "    def metric(y_true, y_pred):\n",
        "        y_true_c = tf.cast(tf.equal(tf.argmax(y_true, -1), class_id), tf.float32)\n",
        "        y_pred_c = tf.cast(tf.equal(tf.argmax(y_pred, -1), class_id), tf.float32)\n",
        "\n",
        "        inter = tf.reduce_sum(y_true_c * y_pred_c)\n",
        "        union = tf.reduce_sum(y_true_c) + tf.reduce_sum(y_pred_c) - inter + 1e-7\n",
        "        return inter / union\n",
        "\n",
        "    metric.__name__ = f'iou_class_{class_id}'\n",
        "    return metric\n",
        "\n",
        "\n",
        "def f1_for_class(class_id):\n",
        "    def metric(y_true, y_pred):\n",
        "        y_true_c = tf.cast(tf.equal(tf.argmax(y_true, -1), class_id), tf.float32)\n",
        "        y_pred_c = tf.cast(tf.equal(tf.argmax(y_pred, -1), class_id), tf.float32)\n",
        "\n",
        "        tp = tf.reduce_sum(y_true_c * y_pred_c)\n",
        "        fp = tf.reduce_sum((1 - y_true_c) * y_pred_c)\n",
        "        fn = tf.reduce_sum(y_true_c * (1 - y_pred_c))\n",
        "\n",
        "        precision = tp / (tp + fp + 1e-7)\n",
        "        recall    = tp / (tp + fn  + 1e-7)\n",
        "        return 2 * precision * recall / (precision + recall + 1e-7)\n",
        "\n",
        "    metric.__name__ = f'f1_class_{class_id}'\n",
        "    return metric"
      ],
      "metadata": {
        "id": "wfeLElZrs50A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = sm.losses.bce_jaccard_loss\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "    loss=loss,\n",
        "    metrics=[\n",
        "        sm.metrics.IOUScore(threshold=None, name=\"iou_score\"),\n",
        "        sm.metrics.FScore(threshold=None, name=\"f1_score\"),\n",
        "        iou_for_class(0),\n",
        "        iou_for_class(1),\n",
        "        f1_for_class(0),\n",
        "        f1_for_class(1),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "VYnPtokytRIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard"
      ],
      "metadata": {
        "id": "Munb4VD7t10h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/Proyecto Integrador/ccpp_rural_only_checkpoints\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_iou_score',\n",
        "    mode='max',\n",
        "    patience=15,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "checkpoint_best = ModelCheckpoint(\n",
        "    filepath=os.path.join(CHECKPOINT_DIR, \"best_unet_rural_only.keras\"),\n",
        "    monitor='val_iou_score',\n",
        "    mode='max',\n",
        "    save_best_only=True\n",
        ")\n",
        "\n",
        "tensorboard_cb = TensorBoard(\n",
        "    log_dir=\"logs/unet_rural_only\"\n",
        ")"
      ],
      "metadata": {
        "id": "zi8OgUA9tRFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/Proyecto Integrador/ccpp_rural_only_checkpoints\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_iou_score',\n",
        "    mode='max',\n",
        "    patience=15,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "checkpoint_best = ModelCheckpoint(\n",
        "    filepath=os.path.join(CHECKPOINT_DIR, \"best_unet_rural_only.keras\"),\n",
        "    monitor='val_iou_score',\n",
        "    mode='max',\n",
        "    save_best_only=True\n",
        ")\n",
        "\n",
        "tensorboard_cb = TensorBoard(\n",
        "    log_dir=\"logs/unet_rural_only\"\n",
        ")"
      ],
      "metadata": {
        "id": "Mw7rcKKftsYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 60\n",
        "\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stop, checkpoint_best, tensorboard_cb]\n",
        ")\n",
        "\n",
        "print(\"Epochs realmente entrenadas:\", len(history.history['loss']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZ7pipP2tRCw",
        "outputId": "6401bab6-57fe-46c1-d684-b0d1235844ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 173ms/step - f1_class_0: 0.8887 - f1_class_1: 0.3832 - f1_score: 0.4898 - iou_class_0: 0.8409 - iou_class_1: 0.2639 - iou_score: 0.4044 - loss: 0.9695 - val_f1_class_0: 0.9661 - val_f1_class_1: 0.0000e+00 - val_f1_score: 0.4869 - val_iou_class_0: 0.9439 - val_iou_class_1: 0.0000e+00 - val_iou_score: 0.4650 - val_loss: 0.6698\n",
            "Epoch 2/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 122ms/step - f1_class_0: 0.9891 - f1_class_1: 0.6482 - f1_score: 0.7402 - iou_class_0: 0.9789 - iou_class_1: 0.5041 - iou_score: 0.6631 - loss: 0.3903 - val_f1_class_0: 0.9796 - val_f1_class_1: 0.4638 - val_f1_score: 0.6968 - val_iou_class_0: 0.9635 - val_iou_class_1: 0.3328 - val_iou_score: 0.6241 - val_loss: 0.4664\n",
            "Epoch 3/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 118ms/step - f1_class_0: 0.9920 - f1_class_1: 0.7259 - f1_score: 0.8336 - iou_class_0: 0.9844 - iou_class_1: 0.5872 - iou_score: 0.7553 - loss: 0.2888 - val_f1_class_0: 0.9876 - val_f1_class_1: 0.5351 - val_f1_score: 0.7430 - val_iou_class_0: 0.9766 - val_iou_class_1: 0.3972 - val_iou_score: 0.6681 - val_loss: 0.3967\n",
            "Epoch 4/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 123ms/step - f1_class_0: 0.9943 - f1_class_1: 0.7766 - f1_score: 0.8726 - iou_class_0: 0.9889 - iou_class_1: 0.6502 - iou_score: 0.8027 - loss: 0.2300 - val_f1_class_0: 0.9839 - val_f1_class_1: 0.5989 - val_f1_score: 0.7819 - val_iou_class_0: 0.9701 - val_iou_class_1: 0.4527 - val_iou_score: 0.7008 - val_loss: 0.3865\n",
            "Epoch 5/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 121ms/step - f1_class_0: 0.9958 - f1_class_1: 0.8154 - f1_score: 0.8978 - iou_class_0: 0.9918 - iou_class_1: 0.7013 - iou_score: 0.8353 - loss: 0.1909 - val_f1_class_0: 0.9885 - val_f1_class_1: 0.5561 - val_f1_score: 0.7685 - val_iou_class_0: 0.9782 - val_iou_class_1: 0.4262 - val_iou_score: 0.6973 - val_loss: 0.3877\n",
            "Epoch 6/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 112ms/step - f1_class_0: 0.9963 - f1_class_1: 0.8513 - f1_score: 0.9181 - iou_class_0: 0.9927 - iou_class_1: 0.7506 - iou_score: 0.8630 - loss: 0.1591 - val_f1_class_0: 0.9796 - val_f1_class_1: 0.5631 - val_f1_score: 0.7659 - val_iou_class_0: 0.9626 - val_iou_class_1: 0.4128 - val_iou_score: 0.6820 - val_loss: 0.4318\n",
            "Epoch 7/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 112ms/step - f1_class_0: 0.9929 - f1_class_1: 0.8427 - f1_score: 0.9118 - iou_class_0: 0.9873 - iou_class_1: 0.7383 - iou_score: 0.8537 - loss: 0.1719 - val_f1_class_0: 0.9890 - val_f1_class_1: 0.5639 - val_f1_score: 0.7738 - val_iou_class_0: 0.9792 - val_iou_class_1: 0.4279 - val_iou_score: 0.7005 - val_loss: 0.3909\n",
            "Epoch 8/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 118ms/step - f1_class_0: 0.9973 - f1_class_1: 0.8729 - f1_score: 0.9308 - iou_class_0: 0.9945 - iou_class_1: 0.7813 - iou_score: 0.8812 - loss: 0.1363 - val_f1_class_0: 0.9887 - val_f1_class_1: 0.5833 - val_f1_score: 0.7830 - val_iou_class_0: 0.9787 - val_iou_class_1: 0.4469 - val_iou_score: 0.7091 - val_loss: 0.3811\n",
            "Epoch 9/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 118ms/step - f1_class_0: 0.9977 - f1_class_1: 0.8906 - f1_score: 0.9409 - iou_class_0: 0.9954 - iou_class_1: 0.8079 - iou_score: 0.8964 - loss: 0.1193 - val_f1_class_0: 0.9887 - val_f1_class_1: 0.5673 - val_f1_score: 0.7762 - val_iou_class_0: 0.9783 - val_iou_class_1: 0.4350 - val_iou_score: 0.7046 - val_loss: 0.4004\n",
            "Epoch 10/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 126ms/step - f1_class_0: 0.9972 - f1_class_1: 0.8906 - f1_score: 0.9404 - iou_class_0: 0.9945 - iou_class_1: 0.8103 - iou_score: 0.8965 - loss: 0.1212 - val_f1_class_0: 0.9892 - val_f1_class_1: 0.5440 - val_f1_score: 0.7648 - val_iou_class_0: 0.9794 - val_iou_class_1: 0.4142 - val_iou_score: 0.6946 - val_loss: 0.4102\n",
            "Epoch 11/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 121ms/step - f1_class_0: 0.9981 - f1_class_1: 0.9052 - f1_score: 0.9492 - iou_class_0: 0.9963 - iou_class_1: 0.8315 - iou_score: 0.9098 - loss: 0.1028 - val_f1_class_0: 0.9891 - val_f1_class_1: 0.5948 - val_f1_score: 0.7904 - val_iou_class_0: 0.9794 - val_iou_class_1: 0.4595 - val_iou_score: 0.7174 - val_loss: 0.3864\n",
            "Epoch 12/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 116ms/step - f1_class_0: 0.9982 - f1_class_1: 0.9134 - f1_score: 0.9534 - iou_class_0: 0.9963 - iou_class_1: 0.8448 - iou_score: 0.9166 - loss: 0.0960 - val_f1_class_0: 0.9890 - val_f1_class_1: 0.6148 - val_f1_score: 0.8000 - val_iou_class_0: 0.9791 - val_iou_class_1: 0.4774 - val_iou_score: 0.7260 - val_loss: 0.3781\n",
            "Epoch 13/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 119ms/step - f1_class_0: 0.9983 - f1_class_1: 0.9201 - f1_score: 0.9571 - iou_class_0: 0.9965 - iou_class_1: 0.8559 - iou_score: 0.9225 - loss: 0.0896 - val_f1_class_0: 0.9894 - val_f1_class_1: 0.6303 - val_f1_score: 0.8081 - val_iou_class_0: 0.9799 - val_iou_class_1: 0.4941 - val_iou_score: 0.7349 - val_loss: 0.3723\n",
            "Epoch 14/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 121ms/step - f1_class_0: 0.9984 - f1_class_1: 0.9283 - f1_score: 0.9614 - iou_class_0: 0.9967 - iou_class_1: 0.8692 - iou_score: 0.9296 - loss: 0.0812 - val_f1_class_0: 0.9885 - val_f1_class_1: 0.5848 - val_f1_score: 0.7854 - val_iou_class_0: 0.9784 - val_iou_class_1: 0.4494 - val_iou_score: 0.7123 - val_loss: 0.4076\n",
            "Epoch 15/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 121ms/step - f1_class_0: 0.9968 - f1_class_1: 0.8948 - f1_score: 0.9428 - iou_class_0: 0.9937 - iou_class_1: 0.8176 - iou_score: 0.9007 - loss: 0.1195 - val_f1_class_0: 0.9887 - val_f1_class_1: 0.6041 - val_f1_score: 0.7942 - val_iou_class_0: 0.9786 - val_iou_class_1: 0.4672 - val_iou_score: 0.7204 - val_loss: 0.3815\n",
            "Epoch 16/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 118ms/step - f1_class_0: 0.9981 - f1_class_1: 0.9261 - f1_score: 0.9601 - iou_class_0: 0.9964 - iou_class_1: 0.8652 - iou_score: 0.9274 - loss: 0.0830 - val_f1_class_0: 0.9888 - val_f1_class_1: 0.6091 - val_f1_score: 0.7973 - val_iou_class_0: 0.9789 - val_iou_class_1: 0.4717 - val_iou_score: 0.7232 - val_loss: 0.3800\n",
            "Epoch 17/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 118ms/step - f1_class_0: 0.9986 - f1_class_1: 0.9320 - f1_score: 0.9636 - iou_class_0: 0.9972 - iou_class_1: 0.8758 - iou_score: 0.9335 - loss: 0.0764 - val_f1_class_0: 0.9887 - val_f1_class_1: 0.5915 - val_f1_score: 0.7891 - val_iou_class_0: 0.9787 - val_iou_class_1: 0.4593 - val_iou_score: 0.7177 - val_loss: 0.4022\n",
            "Epoch 18/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 123ms/step - f1_class_0: 0.9986 - f1_class_1: 0.9450 - f1_score: 0.9703 - iou_class_0: 0.9973 - iou_class_1: 0.8983 - iou_score: 0.9451 - loss: 0.0638 - val_f1_class_0: 0.9875 - val_f1_class_1: 0.5663 - val_f1_score: 0.7761 - val_iou_class_0: 0.9766 - val_iou_class_1: 0.4334 - val_iou_score: 0.7041 - val_loss: 0.4243\n",
            "Epoch 19/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 121ms/step - f1_class_0: 0.9984 - f1_class_1: 0.9341 - f1_score: 0.9646 - iou_class_0: 0.9968 - iou_class_1: 0.8796 - iou_score: 0.9352 - loss: 0.0755 - val_f1_class_0: 0.9886 - val_f1_class_1: 0.5897 - val_f1_score: 0.7873 - val_iou_class_0: 0.9784 - val_iou_class_1: 0.4552 - val_iou_score: 0.7145 - val_loss: 0.3956\n",
            "Epoch 20/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 118ms/step - f1_class_0: 0.9986 - f1_class_1: 0.9348 - f1_score: 0.9650 - iou_class_0: 0.9971 - iou_class_1: 0.8800 - iou_score: 0.9356 - loss: 0.0740 - val_f1_class_0: 0.9894 - val_f1_class_1: 0.6037 - val_f1_score: 0.7953 - val_iou_class_0: 0.9798 - val_iou_class_1: 0.4669 - val_iou_score: 0.7219 - val_loss: 0.3876\n",
            "Epoch 21/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 127ms/step - f1_class_0: 0.9988 - f1_class_1: 0.9435 - f1_score: 0.9697 - iou_class_0: 0.9977 - iou_class_1: 0.8953 - iou_score: 0.9439 - loss: 0.0643 - val_f1_class_0: 0.9893 - val_f1_class_1: 0.6060 - val_f1_score: 0.7967 - val_iou_class_0: 0.9797 - val_iou_class_1: 0.4710 - val_iou_score: 0.7242 - val_loss: 0.3961\n",
            "Epoch 22/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 126ms/step - f1_class_0: 0.9988 - f1_class_1: 0.9490 - f1_score: 0.9726 - iou_class_0: 0.9977 - iou_class_1: 0.9043 - iou_score: 0.9486 - loss: 0.0590 - val_f1_class_0: 0.9892 - val_f1_class_1: 0.5895 - val_f1_score: 0.7884 - val_iou_class_0: 0.9796 - val_iou_class_1: 0.4556 - val_iou_score: 0.7164 - val_loss: 0.3968\n",
            "Epoch 23/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 111ms/step - f1_class_0: 0.9944 - f1_class_1: 0.8903 - f1_score: 0.9387 - iou_class_0: 0.9898 - iou_class_1: 0.8187 - iou_score: 0.8983 - loss: 0.1378 - val_f1_class_0: 0.9888 - val_f1_class_1: 0.5858 - val_f1_score: 0.7848 - val_iou_class_0: 0.9789 - val_iou_class_1: 0.4483 - val_iou_score: 0.7107 - val_loss: 0.3792\n",
            "Epoch 24/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 115ms/step - f1_class_0: 0.9984 - f1_class_1: 0.9363 - f1_score: 0.9654 - iou_class_0: 0.9968 - iou_class_1: 0.8841 - iou_score: 0.9370 - loss: 0.0728 - val_f1_class_0: 0.9891 - val_f1_class_1: 0.6032 - val_f1_score: 0.7951 - val_iou_class_0: 0.9794 - val_iou_class_1: 0.4688 - val_iou_score: 0.7229 - val_loss: 0.3864\n",
            "Epoch 25/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 119ms/step - f1_class_0: 0.9990 - f1_class_1: 0.9522 - f1_score: 0.9743 - iou_class_0: 0.9979 - iou_class_1: 0.9101 - iou_score: 0.9517 - loss: 0.0552 - val_f1_class_0: 0.9892 - val_f1_class_1: 0.6118 - val_f1_score: 0.7994 - val_iou_class_0: 0.9796 - val_iou_class_1: 0.4777 - val_iou_score: 0.7274 - val_loss: 0.3758\n",
            "Epoch 26/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 126ms/step - f1_class_0: 0.9989 - f1_class_1: 0.9588 - f1_score: 0.9777 - iou_class_0: 0.9979 - iou_class_1: 0.9221 - iou_score: 0.9578 - loss: 0.0488 - val_f1_class_0: 0.9890 - val_f1_class_1: 0.5945 - val_f1_score: 0.7910 - val_iou_class_0: 0.9794 - val_iou_class_1: 0.4605 - val_iou_score: 0.7189 - val_loss: 0.4009\n",
            "Epoch 27/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 116ms/step - f1_class_0: 0.9991 - f1_class_1: 0.9590 - f1_score: 0.9779 - iou_class_0: 0.9982 - iou_class_1: 0.9227 - iou_score: 0.9584 - loss: 0.0475 - val_f1_class_0: 0.9889 - val_f1_class_1: 0.5864 - val_f1_score: 0.7870 - val_iou_class_0: 0.9792 - val_iou_class_1: 0.4544 - val_iou_score: 0.7160 - val_loss: 0.4098\n",
            "Epoch 28/60\n",
            "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 126ms/step - f1_class_0: 0.9991 - f1_class_1: 0.9628 - f1_score: 0.9799 - iou_class_0: 0.9982 - iou_class_1: 0.9293 - iou_score: 0.9618 - loss: 0.0439 - val_f1_class_0: 0.9890 - val_f1_class_1: 0.6037 - val_f1_score: 0.7957 - val_iou_class_0: 0.9793 - val_iou_class_1: 0.4693 - val_iou_score: 0.7235 - val_loss: 0.3992\n",
            "Epoch 28: early stopping\n",
            "Restoring model weights from the end of the best epoch: 13.\n",
            "Epochs realmente entrenadas: 28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "TEST_IMG_DIR = os.path.join(DATASET_ROOT, \"test/images\")\n",
        "TEST_MASK_DIR = os.path.join(DATASET_ROOT, \"test/masks\")\n",
        "\n",
        "test_img_paths = sorted(glob.glob(os.path.join(TEST_IMG_DIR, \"*.tif\")))\n",
        "test_mask_paths = sorted(glob.glob(os.path.join(TEST_MASK_DIR, \"*.tif\")))\n",
        "\n",
        "print(\"Test imágenes:\", len(test_img_paths))\n",
        "print(\"Test máscaras:\", len(test_mask_paths))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i99nhOtU1aJG",
        "outputId": "790137e5-83ac-4646-90a7-9360440d702e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test imágenes: 479\n",
            "Test máscaras: 479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_gen = TIFFDataGenerator(\n",
        "    test_img_paths,\n",
        "    test_mask_paths,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    target_size=(512,512),\n",
        "    n_channels=3,\n",
        "    n_classes=2\n",
        ")"
      ],
      "metadata": {
        "id": "dFwRXuKS50DT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics = model.evaluate(test_gen, verbose=1)\n",
        "for name, value in zip(model.metrics_names, test_metrics):\n",
        "    print(f\"{name}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efo7zV006GXT",
        "outputId": "7c2fa21d-4df7-42ba-c028-9fc61e864fcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - f1_class_0: 0.9972 - f1_class_1: 0.5959 - f1_score: 0.7946 - iou_class_0: 0.9944 - iou_class_1: 0.4680 - iou_score: 0.7284 - loss: 0.3081\n",
            "loss: 0.3069067895412445\n",
            "compile_metrics: 0.726443350315094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y_true, w = test_gen[0]\n",
        "y_pred = model.predict(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqO7soAx6GUr",
        "outputId": "2d7f46b3-8eec-4523-cefd-9435db02f3f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n"
          ]
        }
      ]
    }
  ]
}